{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HEOJY88/IMK-AI-POC/blob/main/IMK_PLUS_%ED%92%88%EB%AA%A9%EA%B4%80%EB%A0%A8_POC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0RG2Eo2FK-p",
        "outputId": "72964427-4a34-4671-f0d8-8f6b1a6214ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "hNXwNwUgFbw9",
        "outputId": "33f1cb41-ab48-4a8d-f1b5-655948243505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3626175513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 데이터 전처리 및 분석에 필요한 라이브러리 설치\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pandas'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install scikit-learn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install sentence-transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install chromadb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mmetadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[1;32m    451\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_adapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/email/__init__.py\u001b[0m in \u001b[0;36mmessage_from_string\u001b[0;34m(s, *args, **kws)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \"\"\"\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmessage_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/email/parser.py\u001b[0m in \u001b[0;36mparsestr\u001b[0;34m(self, text, headersonly)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \"\"\"\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheadersonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheadersonly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/email/parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, fp, headersonly)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mfeedparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_headersonly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mfeedparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/email/feedparser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;34m\"\"\"Push more data into the parser.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/email/feedparser.py\u001b[0m in \u001b[0;36m_call_parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/email/feedparser.py\u001b[0m in \u001b[0;36m_parsegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mNeedMoreData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMPTYSTRING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 데이터 전처리 및 분석에 필요한 라이브러리 설치\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install sentence-transformers\n",
        "!pip install chromadb\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9OQeSm9QXqX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# 파일들이 있는 폴더 경로\n",
        "folder_path = '/content/drive/MyDrive/ai_poc_project/zipcode_data/'\n",
        "\n",
        "# 모든 .txt 파일의 경로를 리스트로 가져오기\n",
        "file_paths = glob.glob(folder_path + '*.txt')\n",
        "\n",
        "# 모든 데이터프레임을 담을 빈 리스트\n",
        "df_list = []\n",
        "\n",
        "for file_path in file_paths:\n",
        "    # 한글 인코딩을 지정하여 파일 읽기\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, sep='|', encoding='utf-8', header=0, usecols=['우편번호', '시도', '시군구'])\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(file_path, sep='|', encoding='cp949', header=0, usecols=['우편번호', '시도', '시군구'])\n",
        "\n",
        "    # 우편번호 컬럼을 정수형으로 변환하여 오류 방지\n",
        "    df['우편번호'] = df['우편번호'].astype(str)\n",
        "\n",
        "    df_list.append(df)\n",
        "\n",
        "print(\"모든 파일 로드 완료.\")\n",
        "\n",
        "# 모든 데이터프레임을 하나로 합치기\n",
        "zipcode_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# 중복 데이터 제거 (우편번호 파일에는 중복이 있을 수 있으므로 제거)\n",
        "zipcode_df.drop_duplicates(subset=['우편번호'], inplace=True)\n",
        "zipcode_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(f\"\\n최종 합쳐진 우편번호 데이터 개수: {len(zipcode_df)} 행\")\n",
        "\n",
        "# 최종 데이터프레임을 CSV 파일로 저장\n",
        "# 파일명: all_zipcodes.csv\n",
        "output_file_path = os.path.join(folder_path, 'all_zipcodes.csv')\n",
        "zipcode_df.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"\\n파일이 성공적으로 저장되었습니다: {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4bZUzjHREwK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 파일 저장 경로 설정\n",
        "output_path = '/content/drive/MyDrive/ai_poc_project/dummy_data/'\n",
        "\n",
        "# 폴더가 없으면 생성\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "    print(f\"폴더가 생성되었습니다: {output_path}\")\n",
        "\n",
        "# 1. 주문 및 배송 더미 데이터 저장\n",
        "orders_df.to_csv(os.path.join(output_path, 'dummy_orders.csv'), index=False, encoding='utf-8-sig')\n",
        "\n",
        "# 2. 품목 정보 더미 데이터 저장\n",
        "items_df.to_csv(os.path.join(output_path, 'dummy_items.csv'), index=False, encoding='utf-8-sig')\n",
        "\n",
        "# 3. 우편번호-지역 매핑 더미 데이터 저장\n",
        "# 실제 데이터를 사용할 경우 이 부분은 건너뛰고 실제 파일 경로를 사용합니다.\n",
        "zipcode_df.to_csv(os.path.join(output_path, 'dummy_zipcode_mapping.csv'), index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\n더미 데이터가 CSV 파일로 저장되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRjCOrPCTfpG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# ======================================================================\n",
        "# 1. 데이터 불러오기\n",
        "# ======================================================================\n",
        "\n",
        "# 더미 데이터 경로 (혹은 실제 주문/품목 파일 경로로 수정)\n",
        "dummy_data_path = '/content/drive/MyDrive/ai_poc_project/dummy_data/'\n",
        "orders_df = pd.read_csv(os.path.join(dummy_data_path, 'dummy_orders.csv'))\n",
        "items_df = pd.read_csv(os.path.join(dummy_data_path, 'dummy_items.csv'))\n",
        "\n",
        "# 실제 우편번호 데이터 경로 (이전에 합친 최종 파일 이름으로 수정)\n",
        "real_data_path = '/content/drive/MyDrive/ai_poc_project/zipcode_data/'\n",
        "zipcode_df = pd.read_csv(os.path.join(real_data_path, 'all_zipcodes.csv'))\n",
        "\n",
        "print(\"데이터 로드 완료.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 2. 주문 데이터와 우편번호 데이터 결합\n",
        "# ======================================================================\n",
        "\n",
        "# orders_df와 zipcode_df를 '우편번호' 컬럼을 기준으로 합치기\n",
        "# how='left'를 사용해 모든 주문 데이터를 유지\n",
        "merged_df = pd.merge(orders_df, zipcode_df, on='우편번호', how='left')\n",
        "\n",
        "# 결합 후 새로운 컬럼('시도', '시군구')이 추가되었는지 확인\n",
        "print(\"\\n결합된 데이터프레임의 상위 5개 행:\")\n",
        "print(merged_df.head())\n",
        "\n",
        "# ======================================================================\n",
        "# 3. 새로운 컬럼 생성 (리드 타임, 주문 요일)\n",
        "# ======================================================================\n",
        "\n",
        "# 날짜 컬럼을 datetime 형식으로 변환\n",
        "merged_df['주문일자'] = pd.to_datetime(merged_df['주문일자'])\n",
        "merged_df['배송완료일자'] = pd.to_datetime(merged_df['배송완료일자'])\n",
        "\n",
        "# 리드 타임(배송 소요 시간) 계산 (단위: 일)\n",
        "merged_df['리드 타임'] = (merged_df['배송완료일자'] - merged_df['주문일자']).dt.days\n",
        "\n",
        "# 주문 요일(0: 월요일, 6: 일요일) 계산\n",
        "merged_df['주문 요일'] = merged_df['주문일자'].dt.dayofweek\n",
        "\n",
        "# 최종 데이터프레임의 상위 5개 행과 컬럼 목록 다시 확인\n",
        "print(\"\\n최종 전처리된 데이터프레임의 상위 5개 행:\")\n",
        "print(merged_df.head())\n",
        "print(\"\\n최종 전처리된 데이터프레임의 컬럼 목록:\")\n",
        "print(merged_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv52P5yXbIgR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "\n",
        "# ======================================================================\n",
        "# 1. 필요한 라이브러리 설치\n",
        "# ======================================================================\n",
        "# Colab 세션을 다시 시작했다면 아래 코드를 실행해주세요\n",
        "!pip install -q chromadb sentence-transformers\n",
        "\n",
        "# ======================================================================\n",
        "# 2. 데이터 불러오기\n",
        "# ======================================================================\n",
        "\n",
        "# items_df 불러오기 (상품명, 상품설명을 위해 필요)\n",
        "items_df = pd.read_csv('/content/drive/MyDrive/ai_poc_project/dummy_data/dummy_items.csv')\n",
        "\n",
        "# orders_df 불러오기 (부서 정보(주문자소속팀)를 위해 필요)\n",
        "# 수정된 더미 주문 데이터 파일을 사용합니다.\n",
        "orders_df = pd.read_csv('/content/drive/MyDrive/ai_poc_project/dummy_data/dummy_orders.csv')\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 3. AI 임베딩 모델 로드 및 임베딩 생성\n",
        "# ======================================================================\n",
        "\n",
        "# 한국어에 특화된 경량 임베딩 모델 로드\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "# 품목 정보 텍스트 결합 (임베딩을 위한 입력 텍스트 생성)\n",
        "items_df['embedding_text'] = items_df['품명'] + ' ' + items_df['규격'] + ' ' + items_df['모델명'] + ' ' + items_df['제조원'] + ' ' + items_df['상품설명']\n",
        "\n",
        "# 품목별 임베딩 생성\n",
        "item_embeddings = model.encode(items_df['embedding_text'].tolist(), show_progress_bar=True)\n",
        "print(f\"생성된 품목 임베딩의 형태: {item_embeddings.shape}\")\n",
        "\n",
        "# 부서 임베딩 생성 (유사 부서 분석을 위해)\n",
        "# orders_df에서 '주문자소속팀' 컬럼을 사용해 유일한 부서 목록을 가져옵니다.\n",
        "teams = orders_df['주문자소속팀'].unique().tolist()\n",
        "team_embeddings = model.encode(teams)\n",
        "print(f\"생성된 부서 임베딩의 형태: {team_embeddings.shape}\")\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 4. ChromaDB에 임베딩 저장\n",
        "# ======================================================================\n",
        "\n",
        "# ChromaDB 클라이언트 및 컬렉션 생성 (로컬에서 실행)\n",
        "client = chromadb.Client()\n",
        "\n",
        "# 품목 임베딩을 위한 컬렉션 생성\n",
        "item_collection = client.get_or_create_collection(\"item_embeddings\")\n",
        "item_collection.add(\n",
        "    embeddings=item_embeddings.tolist(),\n",
        "    metadatas=[{\"품목코드\": code} for code in items_df['품목코드'].tolist()],\n",
        "    documents=items_df['embedding_text'].tolist(),\n",
        "    ids=items_df['품목코드'].tolist()\n",
        ")\n",
        "\n",
        "print(\"\\nChromaDB에 품목 임베딩 저장 완료.\")\n",
        "print(f\"ChromaDB에 저장된 데이터 수: {item_collection.count()}\")\n",
        "\n",
        "# 부서 임베딩을 위한 컬렉션 생성\n",
        "team_collection = client.get_or_create_collection(\"team_embeddings\")\n",
        "team_collection.add(\n",
        "    embeddings=team_embeddings.tolist(),\n",
        "    documents=teams,\n",
        "    ids=teams\n",
        ")\n",
        "\n",
        "print(\"\\nChromaDB에 부서 임베딩 저장 완료.\")\n",
        "print(f\"ChromaDB에 저장된 데이터 수: {team_collection.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXePXxXzdP37"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "# ChromaDB 클라이언트 생성\n",
        "client = chromadb.Client()\n",
        "\n",
        "# 품목 임베딩 컬렉션 로드\n",
        "item_collection = client.get_or_create_collection(\"item_embeddings\")\n",
        "\n",
        "# 부서 임베딩 컬렉션 로드\n",
        "team_collection = client.get_or_create_collection(\"team_embeddings\")\n",
        "\n",
        "\n",
        "print(\"===========================\")\n",
        "print(\"  품목 임베딩 컬렉션 확인\")\n",
        "print(f\"  총 데이터 개수: {item_collection.count()}개\")\n",
        "print(\"===========================\\n\")\n",
        "\n",
        "# 품목 컬렉션의 상위 5개 데이터 조회\n",
        "item_results = item_collection.get(limit=5)\n",
        "\n",
        "for i in range(len(item_results['ids'])):\n",
        "    print(f\"ID: {item_results['ids'][i]}\")\n",
        "    print(f\"  메타데이터: {item_results['metadatas'][i]}\")\n",
        "    print(f\"  문서(원본 텍스트): {item_results['documents'][i]}\")\n",
        "    print(\"---------------------------------\")\n",
        "\n",
        "\n",
        "print(\"\\n===========================\")\n",
        "print(\"  부서 임베딩 컬렉션 확인\")\n",
        "print(f\"  총 데이터 개수: {team_collection.count()}개\")\n",
        "print(\"===========================\\n\")\n",
        "\n",
        "# 부서 컬렉션의 모든 데이터 조회\n",
        "team_results = team_collection.get(ids=team_collection.get()['ids'])\n",
        "\n",
        "for i in range(len(team_results['ids'])):\n",
        "    print(f\"ID: {team_results['ids'][i]}\")\n",
        "    print(f\"  문서(원본 텍스트): {team_results['documents'][i]}\")\n",
        "    print(\"---------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRccIKsvucXy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "# ======================================================================\n",
        "# 2. 데이터 불러오기 및 전처리\n",
        "# ======================================================================\n",
        "# orders_df 불러오기 (파일명을 'dummy_orders.csv'로 수정)\n",
        "orders_df = pd.read_csv('/content/drive/MyDrive/ai_poc_project/dummy_data/dummy_orders.csv')\n",
        "\n",
        "# items_df 불러오기\n",
        "items_df = pd.read_csv('/content/drive/MyDrive/ai_poc_project/dummy_data/dummy_items.csv')\n",
        "\n",
        "# 품목코드와 품명을 매핑하는 딕셔너리 생성\n",
        "item_code_to_name = pd.Series(items_df.품명.values, index=items_df.품목코드).to_dict()\n",
        "\n",
        "# ======================================================================\n",
        "# 3. AI 임베딩 생성 및 벡터 DB 구축\n",
        "# ======================================================================\n",
        "# 한국어 임베딩 모델 로드\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "# 품목 임베딩 생성\n",
        "items_df['embedding_text'] = items_df['품명'] + ' ' + items_df['규격'] + ' ' + items_df['모델명'] + ' ' + items_df['제조원'] + ' ' + items_df['상품설명']\n",
        "item_embeddings = model.encode(items_df['embedding_text'].tolist(), show_progress_bar=True)\n",
        "\n",
        "# 부서 임베딩 생성\n",
        "teams = orders_df['주문자소속팀'].unique().tolist()\n",
        "team_embeddings = model.encode(teams)\n",
        "\n",
        "# ChromaDB 클라이언트 및 컬렉션 생성\n",
        "client = chromadb.Client()\n",
        "\n",
        "# 기존 컬렉션이 존재하면 삭제 후 새로 생성 (가장 확실한 초기화 방법)\n",
        "try:\n",
        "    client.delete_collection(name=\"item_embeddings\")\n",
        "    client.delete_collection(name=\"team_embeddings\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "item_collection = client.get_or_create_collection(\"item_embeddings\")\n",
        "team_collection = client.get_or_create_collection(\"team_embeddings\")\n",
        "\n",
        "\n",
        "# 품목 임베딩 저장\n",
        "item_collection.add(\n",
        "    embeddings=item_embeddings.tolist(),\n",
        "    metadatas=[{\"품목코드\": code} for code in items_df['품목코드'].tolist()],\n",
        "    documents=items_df['embedding_text'].tolist(),\n",
        "    ids=items_df['품목코드'].tolist()\n",
        ")\n",
        "\n",
        "# 부서 임베딩 저장\n",
        "team_collection.add(\n",
        "    embeddings=team_embeddings.tolist(),\n",
        "    documents=teams,\n",
        "    ids=teams\n",
        ")\n",
        "print(\"벡터 DB에 품목 및 부서 임베딩 저장 완료.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 4. 분석 함수 정의 및 Gradio UI 구현\n",
        "# ======================================================================\n",
        "def calculate_dot_product_similarity(query_embedding: list, result_embeddings: list):\n",
        "    \"\"\"\n",
        "    쿼리 임베딩과 결과 임베딩 간의 내적 유사도를 계산하는 함수\n",
        "    \"\"\"\n",
        "    query_vector = np.array(query_embedding)\n",
        "    result_vectors = np.array(result_embeddings)\n",
        "    dot_product_scores = np.dot(result_vectors, query_vector)\n",
        "    return dot_product_scores.tolist()\n",
        "\n",
        "\n",
        "def analyze_team_preferences(target_team: str):\n",
        "    \"\"\"\n",
        "    내적 유사도 방식을 적용하여 유사 부서를 분석하는 함수\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 데이터 로드\n",
        "        orders_df = pd.read_csv('/content/drive/MyDrive/ai_poc_project/dummy_data/dummy_orders.csv')\n",
        "        items_df = pd.read_csv('/content/drive/MyDrive/ai_poc_project/dummy_data/dummy_items.csv')\n",
        "        item_code_to_name = pd.Series(items_df.품명.values, index=items_df.품목코드).to_dict()\n",
        "        client = chromadb.Client()\n",
        "        team_collection = client.get_or_create_collection(\"team_embeddings\")\n",
        "\n",
        "        team_sales = orders_df[orders_df['주문자소속팀'] == target_team]\n",
        "        if team_sales.empty:\n",
        "            return f\"[{target_team}]의 판매 데이터가 없습니다.\", \"분석할 유사 부서를 찾을 수 없습니다.\"\n",
        "\n",
        "        top_items = team_sales.groupby('품목코드')['수량'].sum().reset_index()\n",
        "        top_items['품명'] = top_items['품목코드'].map(item_code_to_name)\n",
        "        top_items = top_items.sort_values(by='수량', ascending=False).head(5)\n",
        "\n",
        "        ranking_text = f\"**[{target_team}] 품목 판매 랭킹 (Top 5)**\\n\"\n",
        "        for _, row in top_items.iterrows():\n",
        "            ranking_text += f\"- {row['품명']} ({row['수량']}개)\\n\"\n",
        "\n",
        "        # 벡터 DB에서 타겟 부서의 임베딩 가져오기\n",
        "        target_embedding_result = team_collection.get(ids=[target_team], include=['embeddings'])\n",
        "        if len(target_embedding_result['embeddings']) == 0:\n",
        "            return ranking_text, \"해당 부서의 임베딩이 존재하지 않습니다.\"\n",
        "\n",
        "        target_embedding = np.array(target_embedding_result['embeddings'][0])\n",
        "\n",
        "        # 유사한 부서 찾기 (가장 가까운 3개 부서, 자신 포함)\n",
        "        similar_teams_result = team_collection.query(\n",
        "            query_embeddings=[target_embedding.tolist()],\n",
        "            n_results=4,\n",
        "            include=['documents', 'embeddings']\n",
        "        )\n",
        "\n",
        "        # 문서(원본 텍스트)와 임베딩 정보 추출\n",
        "        results = similar_teams_result['documents'][0]\n",
        "        result_embeddings = similar_teams_result['embeddings'][0]\n",
        "\n",
        "        # --- 내적 유사도 계산 및 정렬 ---\n",
        "        dot_product_scores = calculate_dot_product_similarity(target_embedding.tolist(), result_embeddings)\n",
        "\n",
        "        scored_results = list(zip(dot_product_scores, results))\n",
        "        scored_results.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        similar_teams = [doc for score, doc in scored_results if doc != target_team]\n",
        "        similar_teams = similar_teams[:3] # 상위 3개만 선택\n",
        "\n",
        "        # --- 수정 끝 ---\n",
        "\n",
        "        if not similar_teams:\n",
        "            similar_text = \"유사 부서를 찾을 수 없습니다.\"\n",
        "        else:\n",
        "            similar_teams_df = orders_df[orders_df['주문자소속팀'].isin(similar_teams)]\n",
        "\n",
        "            if similar_teams_df.empty:\n",
        "                similar_text = f\"유사 부서: {', '.join(similar_teams)}\\n\\n유사 부서들의 판매 데이터가 충분하지 않습니다.\"\n",
        "            else:\n",
        "                similar_top_items = similar_teams_df.groupby('품목코드')['수량'].sum().reset_index()\n",
        "                similar_top_items['품명'] = similar_top_items['품목코드'].map(item_code_to_name)\n",
        "                similar_top_items = similar_top_items.sort_values(by='수량', ascending=False).head(5)\n",
        "\n",
        "                similar_text = f\"**[{target_team}]과 유사한 부서들의 선호 품목 (Top 5)**\\n\"\n",
        "                similar_text += f\"(유사 부서: {', '.join(similar_teams)})\\n\"\n",
        "                for _, row in similar_top_items.iterrows():\n",
        "                    similar_text += f\"- {row['품명']} ({row['수량']}개)\\n\"\n",
        "    except Exception as e:\n",
        "        similar_text = f\"유사 부서 분석 중 오류 발생:\\n{traceback.format_exc()}\"\n",
        "\n",
        "    return ranking_text, similar_text\n",
        "\n",
        "# Gradio UI 구현\n",
        "teams = orders_df['주문자소속팀'].unique().tolist()\n",
        "teams.sort()\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# PoC #1: 부서별 품목 선호도 분석\")\n",
        "    gr.Markdown(\"특정 부서를 선택하면, 해당 부서의 판매 랭킹과 AI가 추천하는 유사 부서의 선호 품목을 분석합니다.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        team_dropdown = gr.Dropdown(choices=teams, label=\"분석할 부서를 선택하세요\", value=teams[0])\n",
        "        run_button = gr.Button(\"분석 실행\")\n",
        "\n",
        "    with gr.Row():\n",
        "        ranking_output = gr.Textbox(label=\"선택한 부서의 품목 판매 랭킹\", lines=6)\n",
        "        similar_output = gr.Textbox(label=\"유사 부서들의 선호 품목\", lines=6)\n",
        "\n",
        "    run_button.click(\n",
        "        fn=analyze_team_preferences,\n",
        "        inputs=team_dropdown,\n",
        "        outputs=[ranking_output, similar_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ojr6EIk2nrX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "# ======================================================================\n",
        "# 1. 초기 설정 및 라이브러리 설치\n",
        "# ======================================================================\n",
        "# 요구사항 파일을 생성하고 설치하여 환경을 일관되게 유지\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(\"pandas\\nchromadb\\nsentence-transformers\\ngradio\\n\")\n",
        "!pip install -r requirements.txt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ======================================================================\n",
        "# 2. 데이터 불러오기 및 전처리\n",
        "# ======================================================================\n",
        "# 파일 경로 설정\n",
        "DRIVE_PATH = '/content/drive/MyDrive/ai_poc_project/'\n",
        "DATA_PATH = os.path.join(DRIVE_PATH, 'dummy_data')\n",
        "DB_PATH = os.path.join(DRIVE_PATH, 'db_data')\n",
        "\n",
        "# 데이터 불러오기\n",
        "orders_df = pd.read_csv(os.path.join(DATA_PATH, 'dummy_orders.csv'))\n",
        "items_df = pd.read_csv(os.path.join(DATA_PATH, 'dummy_items.csv'))\n",
        "\n",
        "# 품목코드와 품명을 매핑하는 딕셔너리 생성\n",
        "item_code_to_name = pd.Series(items_df.품명.values, index=items_df.품목코드).to_dict()\n",
        "\n",
        "# ======================================================================\n",
        "# 3. AI 임베딩 생성 및 벡터 DB 구축 (지속성 모드 적용)\n",
        "# ======================================================================\n",
        "# ChromaDB 클라이언트를 지속성 모드로 설정\n",
        "client = chromadb.PersistentClient(path=DB_PATH)\n",
        "\n",
        "# 컬렉션 존재 여부 확인 후 데이터 로드\n",
        "try:\n",
        "    item_collection = client.get_collection(\"item_embeddings\")\n",
        "    team_collection = client.get_collection(\"team_embeddings\")\n",
        "    print(\"기존 벡터 DB 컬렉션을 성공적으로 불러왔습니다.\")\n",
        "\n",
        "except ValueError:\n",
        "    print(\"새로운 벡터 DB를 구축합니다.\")\n",
        "    # 컬렉션이 없으면 새로 생성\n",
        "    item_collection = client.get_or_create_collection(\"item_embeddings\")\n",
        "    team_collection = client.get_or_create_collection(\"team_embeddings\")\n",
        "\n",
        "    # 한국어 임베딩 모델 로드\n",
        "    model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "    # 품목 임베딩 생성\n",
        "    items_df['embedding_text'] = items_df['품명'] + ' ' + items_df['규격'] + ' ' + items_df['모델명'] + ' ' + items_df['제조원'] + ' ' + items_df['상품설명']\n",
        "    item_embeddings = model.encode(items_df['embedding_text'].tolist(), show_progress_bar=True)\n",
        "\n",
        "    # 부서 임베딩 생성\n",
        "    teams = orders_df['주문자소속팀'].unique().tolist()\n",
        "    team_embeddings = model.encode(teams)\n",
        "\n",
        "    # 품목 임베딩 저장\n",
        "    item_collection.add(\n",
        "        embeddings=item_embeddings.tolist(),\n",
        "        metadatas=[{\"품목코드\": code} for code in items_df['품목코드'].tolist()],\n",
        "        documents=items_df['embedding_text'].tolist(),\n",
        "        ids=items_df['품목코드'].tolist()\n",
        "    )\n",
        "\n",
        "    # 부서 임베딩 저장\n",
        "    team_collection.add(\n",
        "        embeddings=team_embeddings.tolist(),\n",
        "        documents=teams,\n",
        "        ids=teams\n",
        "    )\n",
        "    print(\"새로운 벡터 DB에 품목 및 부서 임베딩 저장 완료.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 4. 분석 함수 정의 및 Gradio UI 구현\n",
        "# ======================================================================\n",
        "def calculate_dot_product_similarity(query_embedding: list, result_embeddings: list):\n",
        "    \"\"\"\n",
        "    쿼리 임베딩과 결과 임베딩 간의 내적 유사도를 계산하는 함수\n",
        "    \"\"\"\n",
        "    query_vector = np.array(query_embedding)\n",
        "    result_vectors = np.array(result_embeddings)\n",
        "    dot_product_scores = np.dot(result_vectors, query_vector)\n",
        "    return dot_product_scores.tolist()\n",
        "\n",
        "\n",
        "def analyze_team_preferences(target_team: str):\n",
        "    \"\"\"\n",
        "    내적 유사도 방식을 적용하여 유사 부서를 분석하는 함수\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 데이터 로드\n",
        "        orders_df = pd.read_csv(os.path.join(DATA_PATH, 'dummy_orders.csv'))\n",
        "        items_df = pd.read_csv(os.path.join(DATA_PATH, 'dummy_items.csv'))\n",
        "        item_code_to_name = pd.Series(items_df.품명.values, index=items_df.품목코드).to_dict()\n",
        "\n",
        "        team_sales = orders_df[orders_df['주문자소속팀'] == target_team]\n",
        "        if team_sales.empty:\n",
        "            return f\"[{target_team}]의 판매 데이터가 없습니다.\", \"분석할 유사 부서를 찾을 수 없습니다.\"\n",
        "\n",
        "        top_items = team_sales.groupby('품목코드')['수량'].sum().reset_index()\n",
        "        top_items['품명'] = top_items['품목코드'].map(item_code_to_name)\n",
        "        top_items = top_items.sort_values(by='수량', ascending=False).head(5)\n",
        "\n",
        "        ranking_text = f\"**[{target_team}] 품목 판매 랭킹 (Top 5)**\\n\"\n",
        "        for _, row in top_items.iterrows():\n",
        "            ranking_text += f\"- {row['품명']} ({row['수량']}개)\\n\"\n",
        "\n",
        "        # 벡터 DB에서 타겟 부서의 임베딩 가져오기\n",
        "        target_embedding_result = team_collection.get(ids=[target_team], include=['embeddings'])\n",
        "        if not target_embedding_result['embeddings']:\n",
        "            return ranking_text, \"해당 부서의 임베딩이 존재하지 않습니다.\"\n",
        "\n",
        "        target_embedding = np.array(target_embedding_result['embeddings'][0])\n",
        "\n",
        "        # 유사한 부서 찾기 (가장 가까운 3개 부서, 자신 포함)\n",
        "        similar_teams_result = team_collection.query(\n",
        "            query_embeddings=[target_embedding.tolist()],\n",
        "            n_results=4,\n",
        "            include=['documents', 'embeddings']\n",
        "        )\n",
        "\n",
        "        # 문서(원본 텍스트)와 임베딩 정보 추출\n",
        "        results = similar_teams_result['documents'][0]\n",
        "        result_embeddings = similar_teams_result['embeddings'][0]\n",
        "\n",
        "        # --- 내적 유사도 계산 및 정렬 ---\n",
        "        dot_product_scores = calculate_dot_product_similarity(target_embedding.tolist(), result_embeddings)\n",
        "\n",
        "        scored_results = sorted(zip(dot_product_scores, results), key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        similar_teams = [doc for score, doc in scored_results if doc != target_team][:3]\n",
        "\n",
        "        # --- 수정 끝 ---\n",
        "\n",
        "        if not similar_teams:\n",
        "            similar_text = \"유사 부서를 찾을 수 없습니다.\"\n",
        "        else:\n",
        "            similar_teams_df = orders_df[orders_df['주문자소속팀'].isin(similar_teams)]\n",
        "\n",
        "            if similar_teams_df.empty:\n",
        "                similar_text = f\"유사 부서: {', '.join(similar_teams)}\\n\\n유사 부서들의 판매 데이터가 충분하지 않습니다.\"\n",
        "            else:\n",
        "                similar_top_items = similar_teams_df.groupby('품목코드')['수량'].sum().reset_index()\n",
        "                similar_top_items['품명'] = similar_top_items['품목코드'].map(item_code_to_name)\n",
        "                similar_top_items = similar_top_items.sort_values(by='수량', ascending=False).head(5)\n",
        "\n",
        "                similar_text = f\"**[{target_team}]과 유사한 부서들의 선호 품목 (Top 5)**\\n\"\n",
        "                similar_text += f\"(유사 부서: {', '.join(similar_teams)})\\n\"\n",
        "                for _, row in similar_top_items.iterrows():\n",
        "                    similar_text += f\"- {row['품명']} ({row['수량']}개)\\n\"\n",
        "    except Exception as e:\n",
        "        ranking_text = \"오류 발생\"\n",
        "        similar_text = f\"유사 부서 분석 중 오류 발생:\\n{traceback.format_exc()}\"\n",
        "\n",
        "    return ranking_text, similar_text\n",
        "\n",
        "# Gradio UI 구현\n",
        "teams = sorted(orders_df['주문자소속팀'].unique().tolist())\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# PoC #1: 부서별 품목 선호도 분석\")\n",
        "    gr.Markdown(\"특정 부서를 선택하면, 해당 부서의 판매 랭킹과 AI가 추천하는 유사 부서의 선호 품목을 분석합니다.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        team_dropdown = gr.Dropdown(choices=teams, label=\"분석할 부서를 선택하세요\", value=teams[0])\n",
        "        run_button = gr.Button(\"분석 실행\", variant=\"primary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        ranking_output = gr.Textbox(label=\"선택한 부서의 품목 판매 랭킹\", lines=6)\n",
        "        similar_output = gr.Textbox(label=\"유사 부서들의 선호 품목\", lines=6)\n",
        "\n",
        "    run_button.click(\n",
        "        fn=analyze_team_preferences,\n",
        "        inputs=team_dropdown,\n",
        "        outputs=[ranking_output, similar_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcLX7nFu3NA7"
      },
      "source": [
        "여기서 부터 다시 해보자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knbZKpcs3OuK"
      },
      "outputs": [],
      "source": [
        "# 1. Google Drive 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. 필수 라이브러리 설치\n",
        "!pip install pandas chromadb sentence-transformers gradio scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vivOn5uI3PfP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import chromadb\n",
        "import os\n",
        "\n",
        "# 파일 경로 설정\n",
        "DRIVE_PATH = '/content/drive/MyDrive/ai_poc_project/'\n",
        "DATA_PATH = os.path.join(DRIVE_PATH, 'dummy_data')\n",
        "DB_PATH = os.path.join(DRIVE_PATH, 'db_data')\n",
        "\n",
        "# 데이터 불러오기\n",
        "orders_df = pd.read_csv(os.path.join(DATA_PATH, 'dummy_orders.csv'))\n",
        "items_df = pd.read_csv(os.path.join(DATA_PATH, 'dummy_items.csv'))\n",
        "item_code_to_name = pd.Series(items_df.품명.values, index=items_df.품목코드).to_dict()\n",
        "\n",
        "# 지속성 모드로 ChromaDB 클라이언트 연결\n",
        "client = chromadb.PersistentClient(path=DB_PATH)\n",
        "\n",
        "# ==================== 수정된 부분 ====================\n",
        "# .get_collection -> .get_or_create_collection 으로 변경\n",
        "team_collection = client.get_or_create_collection(\"team_embeddings\")\n",
        "# ====================================================\n",
        "\n",
        "print(\"✅ 데이터 로드 및 벡터 DB 연결 완료!\")\n",
        "print(f\"팀 컬렉션의 데이터 수: {team_collection.count()}개\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD1SZleG7wRv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "# ======================================================================\n",
        "# 1. 환경 설정: Google Drive 연결 및 라이브러리 설치\n",
        "# ======================================================================\n",
        "print(\"1. 환경 설정을 시작합니다...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install pandas chromadb sentence-transformers gradio -q\n",
        "print(\"✅ 라이브러리 설치 완료!\")\n",
        "\n",
        "# ======================================================================\n",
        "# 2. 데이터 및 경로 설정\n",
        "# ======================================================================\n",
        "print(\"\\n2. 데이터 및 경로를 설정합니다...\")\n",
        "# 파일 경로 설정\n",
        "DRIVE_PATH = '/content/drive/MyDrive/ai_poc_project/'\n",
        "DATA_PATH = os.path.join(DRIVE_PATH, 'dummy_data')\n",
        "DB_PATH = os.path.join(DRIVE_PATH, 'db_data')\n",
        "\n",
        "# 데이터 불러오기\n",
        "orders_df = pd.read_csv(os.path.join(DATA_PATH, 'dummy_orders.csv'))\n",
        "items_df = pd.read_csv(os.path.join(DATA_PATH, 'dummy_items.csv'))\n",
        "print(\"✅ 데이터 로드 완료!\")\n",
        "\n",
        "# ======================================================================\n",
        "# 3. AI 임베딩 생성 및 영구 벡터 DB 구축\n",
        "# ======================================================================\n",
        "print(\"\\n3. AI 임베딩 생성 및 영구 벡터 DB 구축을 시작합니다...\")\n",
        "# ChromaDB 클라이언트를 지속성 모드로 설정\n",
        "client = chromadb.PersistentClient(path=DB_PATH)\n",
        "\n",
        "# 기존 컬렉션이 있다면 삭제하여 초기화 (최초 실행 시 안전 장치)\n",
        "try:\n",
        "    client.delete_collection(name=\"item_embeddings\")\n",
        "    client.delete_collection(name=\"team_embeddings\")\n",
        "    print(\"기존 컬렉션을 초기화했습니다.\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 새로운 컬렉션 생성\n",
        "item_collection = client.get_or_create_collection(\"item_embeddings\")\n",
        "team_collection = client.get_or_create_collection(\"team_embeddings\")\n",
        "\n",
        "# 한국어 임베딩 모델 로드\n",
        "print(\"임베딩 모델을 로드합니다... (시간이 소요될 수 있습니다)\")\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "# 품목 임베딩 생성\n",
        "print(\"품목 임베딩을 생성합니다...\")\n",
        "items_df['embedding_text'] = items_df['품명'] + ' ' + items_df['규격'] + ' ' + items_df['모델명'] + ' ' + items_df['제조원'] + ' ' + items_df['상품설명']\n",
        "item_embeddings = model.encode(items_df['embedding_text'].tolist(), show_progress_bar=True)\n",
        "\n",
        "# 부서 임베딩 생성\n",
        "print(\"부서 임베딩을 생성합니다...\")\n",
        "teams = orders_df['주문자소속팀'].unique().tolist()\n",
        "team_embeddings = model.encode(teams)\n",
        "\n",
        "# 품목 임베딩 저장\n",
        "print(\"생성된 품목 임베딩을 영구 DB에 저장합니다...\")\n",
        "item_collection.add(\n",
        "    embeddings=item_embeddings.tolist(),\n",
        "    metadatas=[{\"품목코드\": code} for code in items_df['품목코드'].tolist()],\n",
        "    documents=items_df['embedding_text'].tolist(),\n",
        "    ids=items_df['품목코드'].tolist()\n",
        ")\n",
        "\n",
        "# 부서 임베딩 저장\n",
        "print(\"생성된 부서 임베딩을 영구 DB에 저장합니다...\")\n",
        "team_collection.add(\n",
        "    embeddings=team_embeddings.tolist(),\n",
        "    documents=teams,\n",
        "    ids=teams\n",
        ")\n",
        "\n",
        "print(\"\\n✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨\")\n",
        "print(\"✅ 영구 벡터 DB 생성 및 데이터 저장 완료!\")\n",
        "print(f\"품목 데이터 수: {item_collection.count()}개\")\n",
        "print(f\"부서 데이터 수: {team_collection.count()}개\")\n",
        "print(\"✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "winWRR_38aHY"
      },
      "source": [
        "여기서 테스트를 위해 런타임 종료\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1nGsBqC8cX0"
      },
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# 1단계: 환경 설정 (Drive 연결 및 라이브러리 설치)\n",
        "# ======================================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install pandas chromadb sentence-transformers gradio -q\n",
        "print(\"✅ 1단계: 환경 설정 완료!\")\n",
        "\n",
        "# ======================================================================\n",
        "# 2단계: 데이터 로드 및 영구 벡터 DB 연결\n",
        "# ======================================================================\n",
        "import pandas as pd\n",
        "import chromadb\n",
        "import os\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/ai_poc_project/'\n",
        "DATA_PATH = os.path.join(DRIVE_PATH, 'dummy_data')\n",
        "DB_PATH = os.path.join(DRIVE_PATH, 'db_data')\n",
        "\n",
        "orders_df = pd.read_csv(os.path.join(DATA_PATH, 'dummy_orders.csv'))\n",
        "items_df = pd.read_csv(os.path.join(DATA_PATH, 'dummy_items.csv'))\n",
        "item_code_to_name = pd.Series(items_df.품명.values, index=items_df.품목코드).to_dict()\n",
        "\n",
        "client = chromadb.PersistentClient(path=DB_PATH)\n",
        "team_collection = client.get_collection(\"team_embeddings\") # 이제 get_or_create가 아닌 get을 사용해도 됩니다.\n",
        "\n",
        "print(f\"✅ 2단계: 데이터 로드 및 벡터 DB 연결 완료! (팀 데이터 수: {team_collection.count()}개)\")\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 3단계: 분석 함수 및 Gradio UI 실행 (수정 완료)\n",
        "# ======================================================================\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import traceback\n",
        "\n",
        "def calculate_dot_product_similarity(query_embedding: list, result_embeddings: list):\n",
        "    query_vector = np.array(query_embedding)\n",
        "    result_vectors = np.array(result_embeddings)\n",
        "    dot_product_scores = np.dot(result_vectors, query_vector)\n",
        "    return dot_product_scores.tolist()\n",
        "\n",
        "def analyze_team_preferences(target_team: str):\n",
        "    try:\n",
        "        team_sales = orders_df[orders_df['주문자소속팀'] == target_team]\n",
        "        if team_sales.empty: return f\"[{target_team}]의 판매 데이터가 없습니다.\", \"분석할 유사 부서를 찾을 수 없습니다.\"\n",
        "        top_items = team_sales.groupby('품목코드')['수량'].sum().reset_index()\n",
        "        top_items['품명'] = top_items['품목코드'].map(item_code_to_name)\n",
        "        top_items = top_items.sort_values(by='수량', ascending=False).head(5)\n",
        "        ranking_text = f\"**[{target_team}] 품목 판매 랭킹 (Top 5)**\\n\"\n",
        "        for _, row in top_items.iterrows(): ranking_text += f\"- {row['품명']} ({row['수량']}개)\\n\"\n",
        "\n",
        "        target_embedding_result = team_collection.get(ids=[target_team], include=['embeddings'])\n",
        "\n",
        "        # ==================== 수정된 부분 ====================\n",
        "        if len(target_embedding_result['embeddings']) == 0:\n",
        "        # ====================================================\n",
        "            return ranking_text, \"해당 부서의 임베딩이 존재하지 않습니다.\"\n",
        "\n",
        "        target_embedding = np.array(target_embedding_result['embeddings'][0])\n",
        "        similar_teams_result = team_collection.query(query_embeddings=[target_embedding.tolist()], n_results=4, include=['documents', 'embeddings'])\n",
        "        results, result_embeddings = similar_teams_result['documents'][0], similar_teams_result['embeddings'][0]\n",
        "        dot_product_scores = calculate_dot_product_similarity(target_embedding.tolist(), result_embeddings)\n",
        "        scored_results = sorted(zip(dot_product_scores, results), key=lambda x: x[0], reverse=True)\n",
        "        similar_teams = [doc for score, doc in scored_results if doc != target_team][:3]\n",
        "        if not similar_teams: similar_text = \"유사 부서를 찾을 수 없습니다.\"\n",
        "        else:\n",
        "            similar_teams_df = orders_df[orders_df['주문자소속팀'].isin(similar_teams)]\n",
        "            if similar_teams_df.empty: similar_text = f\"유사 부서: {', '.join(similar_teams)}\\n\\n유사 부서들의 판매 데이터가 충분하지 않습니다.\"\n",
        "            else:\n",
        "                similar_top_items = similar_teams_df.groupby('품목코드')['수량'].sum().reset_index()\n",
        "                similar_top_items['품명'] = similar_top_items['품목코드'].map(item_code_to_name)\n",
        "                similar_top_items = similar_top_items.sort_values(by='수량', ascending=False).head(5)\n",
        "                similar_text = f\"**[{target_team}]과 유사한 부서들의 선호 품목 (Top 5)**\\n\"\n",
        "                similar_text += f\"(유사 부서: {', '.join(similar_teams)})\\n\"\n",
        "                for _, row in similar_top_items.iterrows(): similar_text += f\"- {row['품명']} ({row['수량']}개)\\n\"\n",
        "    except Exception as e:\n",
        "        ranking_text, similar_text = \"오류 발생\", f\"분석 중 오류 발생:\\n{traceback.format_exc()}\"\n",
        "    return ranking_text, similar_text\n",
        "\n",
        "teams = sorted(orders_df['주문자소속팀'].unique().tolist())\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# PoC #1: 부서별 품목 선호도 분석\")\n",
        "    gr.Markdown(\"특정 부서를 선택하면, 해당 부서의 판매 랭킹과 AI가 추천하는 유사 부서의 선호 품목을 분석합니다.\")\n",
        "    with gr.Row():\n",
        "        team_dropdown = gr.Dropdown(choices=teams, label=\"분석할 부서를 선택하세요\", value=teams[0])\n",
        "        run_button = gr.Button(\"분석 실행\", variant=\"primary\")\n",
        "    with gr.Row():\n",
        "        ranking_output = gr.Textbox(label=\"선택한 부서의 품목 판매 랭킹\", lines=6)\n",
        "        similar_output = gr.Textbox(label=\"유사 부서들의 선호 품목\", lines=6)\n",
        "    run_button.click(fn=analyze_team_preferences, inputs=team_dropdown, outputs=[ranking_output, similar_output])\n",
        "\n",
        "print(\"✅ 3단계: Gradio UI를 실행합니다...\")\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPpI_LwyNcLg"
      },
      "source": [
        "1.   잠정적 POC # 1 작업 완료 (주문 데이터 생성 필요)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKyvZKXXNdjR"
      },
      "source": [
        "POC #2\n",
        "1. 품목 정보 백터 DB 화\n",
        "2. 네이버 상품 API 연동을 통한 품목 정비\n",
        "3. 유사 상품 추천 로직 개발\n",
        "4. UI 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xSefJNARUe3"
      },
      "outputs": [],
      "source": [
        "# 필수 라이브러리 설치\n",
        "!pip install pandas chromadb sentence-transformers -q\n",
        "\n",
        "print(\"✅ 라이브러리 설치 완료!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErWfKW_dcfMv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "\n",
        "# ======================================================================\n",
        "# 0. 환경 설정 및 데이터 로드\n",
        "# ======================================================================\n",
        "# 이전 단계에서 API 정보로 보강이 완료된 데이터 파일 경로\n",
        "file_path = '/content/drive/MyDrive/ai_poc_project/original_data/item_data_enriched.csv'\n",
        "\n",
        "try:\n",
        "    # Google Drive 연결\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # 라이브러리 설치\n",
        "    !pip install pandas chromadb sentence-transformers -q\n",
        "\n",
        "    enriched_df = pd.read_csv(file_path)\n",
        "    print(f\"✅ 총 {len(enriched_df)}개 보강된 품목 데이터 로드 완료.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ [에러] '{file_path}' 파일을 찾을 수 없습니다.\")\n",
        "    print(\"이전 단계의 데이터 보강 코드가 성공적으로 실행되었는지 확인해주세요.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ [에러] 파일을 읽는 중 오류가 발생했습니다: {e}\")\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 1. \"AI 학습 자료\" (embedding_text) 최종 재구성\n",
        "# ======================================================================\n",
        "print(\"\\n1. 모든 정보를 반영하여 AI 학습 자료를 최종 구성합니다...\")\n",
        "\n",
        "# 학습에 사용할 전체 컬럼 리스트\n",
        "cols_to_embed = [\n",
        "    '품목코드', '한글품명', '영문품명', '규격', '모델명', '제조원명',\n",
        "    '네이버_카테고리1', '네이버_카테고리2', '네이버_카테고리3', '네이버_카테고리4',\n",
        "    '브랜드', '최저가'\n",
        "]\n",
        "\n",
        "# 빈 값(NaN)을 안전하게 처리하고, 모든 컬럼을 문자열로 변환하여 결합\n",
        "enriched_df['embedding_text'] = enriched_df[cols_to_embed].fillna('').astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "print(\"✅ 최종 embedding_text 생성 완료!\")\n",
        "print(\"\\n[생성된 embedding_text 샘플]\")\n",
        "display(enriched_df[['품목코드', 'embedding_text']].head())\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 2. 최종 벡터 생성 및 영구 DB 업데이트\n",
        "# ======================================================================\n",
        "print(\"\\n2. 최종 벡터 생성 및 영구 DB 업데이트를 시작합니다...\")\n",
        "\n",
        "# 영구 DB 경로 설정 및 클라이언트 연결\n",
        "DB_PATH = '/content/drive/MyDrive/ai_poc_project/db_data'\n",
        "client = chromadb.PersistentClient(path=DB_PATH)\n",
        "\n",
        "# AI 임베딩 모델 로드\n",
        "model_name = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n",
        "print(f\"   - AI 임베딩 모델({model_name})을 로드합니다...\")\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# 최종 품목별 임베딩 생성\n",
        "print(\"   - 모든 정보가 포함된 벡터를 생성합니다... (시간이 소요될 수 있습니다)\")\n",
        "item_embeddings = model.encode(enriched_df['embedding_text'].tolist(), show_progress_bar=True)\n",
        "\n",
        "# 기존 컬렉션 삭제 (초기화)\n",
        "try:\n",
        "    client.delete_collection(name=\"item_embeddings\")\n",
        "    print(\"   - 기존 item_embeddings 컬렉션을 성공적으로 삭제했습니다.\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 새로운 컬렉션 생성 및 최종 데이터 저장\n",
        "print(\"   - 최종 벡터를 영구 DB에 저장합니다...\")\n",
        "item_collection = client.get_or_create_collection(\"item_embeddings\")\n",
        "item_collection.add(\n",
        "    embeddings=item_embeddings.tolist(),\n",
        "    metadatas=[{\"품목코드\": code} for code in enriched_df['품목코드'].tolist()],\n",
        "    documents=enriched_df['embedding_text'].tolist(),\n",
        "    ids=[str(code) for code in enriched_df['품목코드'].tolist()]\n",
        ")\n",
        "\n",
        "print(\"\\n✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨\")\n",
        "print(\"✅ [성공] 모든 정보가 반영된 최종 벡터 DB 구축 완료!\")\n",
        "print(f\"✅ 최종 저장된 품목 데이터 수: {item_collection.count()}개\")\n",
        "print(\"✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsNGTu9ltFAR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 0. 환경 설정 및 데이터 로드 ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except Exception: pass\n",
        "!pip install tqdm -q\n",
        "file_path = '/content/drive/MyDrive/ai_poc_project/original_data/item_data.txt'\n",
        "real_items_df = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "# --- 1. Naver API 설정 ---\n",
        "CLIENT_ID = \"1uypXVdrAyuahn7jp05g\"\n",
        "CLIENT_SECRET = \"zzWqd49IJv\"\n",
        "headers = {\"X-Naver-Client-Id\": CLIENT_ID, \"X-Naver-Client-Secret\": CLIENT_SECRET}\n",
        "\n",
        "# --- 2. API 호출 및 데이터 보강 (수정된 로직) ---\n",
        "print(\"\\nNaver API를 통해 데이터 보강을 시작합니다...\")\n",
        "api_results = []\n",
        "for index, row in tqdm(real_items_df.iterrows(), total=real_items_df.shape[0]):\n",
        "    query = row['한글품명']\n",
        "    url = f\"https://openapi.naver.com/v1/search/shop.json?query={query}&display=1\"\n",
        "\n",
        "    result = { # 모든 키를 미리 정의하여 KeyError 방지\n",
        "        '네이버_카테고리1': 'N/A', '네이버_카테고리2': 'N/A', '네이버_카테고리3': 'N/A',\n",
        "        '네이버_카테고리4': 'N/A', '브랜드': 'N/A', '이미지_URL': 'N/A',\n",
        "        '최저가': 0, '상품_링크': 'N/A'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200 and response.json()['items']:\n",
        "            item = response.json()['items'][0]\n",
        "            result.update({\n",
        "                '네이버_카테고리1': item.get('category1', 'N/A'),\n",
        "                '네이버_카테고리2': item.get('category2', 'N/A'),\n",
        "                '네이버_카테고리3': item.get('category3', 'N/A'),\n",
        "                '네이버_카테고리4': item.get('category4', 'N/A'),\n",
        "                '브랜드': item.get('brand', 'N/A'),\n",
        "                '이미지_URL': item.get('image', 'N/A'),\n",
        "                '최저가': int(item.get('lprice', 0)),\n",
        "                '상품_링크': item.get('link', 'N/A')\n",
        "            })\n",
        "    except Exception: pass\n",
        "    api_results.append(result)\n",
        "    time.sleep(0.05)\n",
        "\n",
        "enriched_df = pd.concat([real_items_df.reset_index(drop=True), pd.DataFrame(api_results)], axis=1)\n",
        "output_path = '/content/drive/MyDrive/ai_poc_project/original_data/item_data_enriched.csv'\n",
        "enriched_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\n✅ 보강된 데이터가 정상적으로 저장되었습니다:\\n{output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NrA7omsQ8uM"
      },
      "outputs": [],
      "source": [
        "위에 품목 정비는 월요일에 한번 더 수행하자\n",
        "\n",
        "아래 소스는 POC1,2차 통합 소스 . 2차 유사 상품은 실시간 검색이고, 유사도는 0.75 이상만 추출 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb93seN93LMn"
      },
      "source": [
        "지속성을 남기는 방식에서 크롬 DB 와 클라우드 환경 간 이슈 발생 하여 방향성을 바꿔보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsiLBEqy3Q5U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- 0. 환경 설정 및 데이터 로드 ---\n",
        "print(\"환경 설정 및 데이터 로드를 시작합니다...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except Exception: pass\n",
        "!pip install pandas chromadb sentence-transformers -q\n",
        "\n",
        "file_path = '/content/drive/MyDrive/ai_poc_project/original_data/item_data_enriched.csv'\n",
        "try:\n",
        "    enriched_df = pd.read_csv(file_path)\n",
        "    print(f\"✅ 총 {len(enriched_df)}개 보강된 품목 데이터 로드 완료.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ [에러] 파일을 읽는 중 오류가 발생했습니다: {e}\")\n",
        "    raise SystemExit(\"데이터 로드 실패.\")\n",
        "\n",
        "# --- 1. \"AI 학습 자료\" (embedding_text) 최종 재구성 ---\n",
        "print(\"\\n1. 모든 정보를 반영하여 AI 학습 자료를 최종 구성합니다...\")\n",
        "cols_to_embed = [\n",
        "    '품목코드', '한글품명', '영문품명', '규격', '모델명', '제조원명',\n",
        "    '네이버_카테고리1', '네이버_카테고리2', '네이버_카테고리3', '네이버_카테고리4',\n",
        "    '브랜드', '최저가'\n",
        "]\n",
        "enriched_df['embedding_text'] = enriched_df[cols_to_embed].fillna('').astype(str).agg(' '.join, axis=1)\n",
        "print(\"✅ 최종 embedding_text 생성 완료!\")\n",
        "\n",
        "# --- 2. 최종 벡터 생성 및 \"로컬 DB\" 업데이트 ---\n",
        "print(\"\\n2. 최종 벡터 생성 및 '로컬 DB' 업데이트를 시작합니다...\")\n",
        "# ======== 저장 경로 변경 ========\n",
        "DB_PATH = '/content/db_data' # Google Drive가 아닌 Colab 로컬 경로\n",
        "# ============================\n",
        "\n",
        "if os.path.exists(DB_PATH):\n",
        "    shutil.rmtree(DB_PATH)\n",
        "os.makedirs(DB_PATH, exist_ok=True)\n",
        "\n",
        "client = chromadb.PersistentClient(path=DB_PATH)\n",
        "model_name = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n",
        "print(f\"   - AI 임베딩 모델({model_name})을 로드합니다...\")\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "print(\"   - 모든 정보가 포함된 벡터를 생성합니다... (시간이 소요될 수 있습니다)\")\n",
        "item_embeddings = model.encode(enriched_df['embedding_text'].tolist(), show_progress_bar=True)\n",
        "\n",
        "item_collection = client.get_or_create_collection(\"item_embeddings\")\n",
        "item_collection.add(\n",
        "    embeddings=item_embeddings.tolist(),\n",
        "    metadatas=[{\"품목코드\": code} for code in enriched_df['품목코드'].tolist()],\n",
        "    documents=enriched_df['embedding_text'].tolist(),\n",
        "    ids=[str(code) for code in enriched_df['품목코드'].tolist()]\n",
        ")\n",
        "\n",
        "print(\"\\n✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨\")\n",
        "print(f\"✅ [성공] 로컬 벡터 DB 재구축 완료! (경로: {DB_PATH})\")\n",
        "print(f\"✅ 최종 저장된 품목 데이터 수: {item_collection.count()}개\")\n",
        "print(\"✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmK4YN805PFl"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# --- 경로 설정 ---\n",
        "local_db_path = '/content/db_data'\n",
        "backup_drive_folder = '/content/drive/MyDrive/ai_poc_project/db_backup'\n",
        "# --- 경로 설정 끝 ---\n",
        "\n",
        "print(f\"'{local_db_path}' 경로의 로컬 DB를 Google Drive에 백업합니다...\")\n",
        "\n",
        "try:\n",
        "    os.makedirs(backup_drive_folder, exist_ok=True)\n",
        "    backup_filename = f\"db_backup_{pd.Timestamp.now().strftime('%Y%m%d')}\"\n",
        "\n",
        "    # 압축할 파일의 전체 경로\n",
        "    zip_file_path_local = f\"{backup_filename}.zip\"\n",
        "    zip_file_path_drive = os.path.join(backup_drive_folder, zip_file_path_local)\n",
        "\n",
        "    # 1. 로컬 DB 폴더를 zip 파일로 압축\n",
        "    shutil.make_archive(backup_filename, 'zip', local_db_path)\n",
        "    print(f\"   - 로컬 DB를 '{zip_file_path_local}' 파일로 압축했습니다.\")\n",
        "\n",
        "    # ======== 수정된 부분 ========\n",
        "    # 2. 만약 Google Drive에 동일한 이름의 파일이 이미 있다면, 먼저 삭제\n",
        "    if os.path.exists(zip_file_path_drive):\n",
        "        os.remove(zip_file_path_drive)\n",
        "        print(f\"   - 기존 백업 파일({zip_file_path_drive})을 삭제했습니다.\")\n",
        "    # ============================\n",
        "\n",
        "    # 3. 새로 생성된 zip 파일을 Google Drive 백업 폴더로 이동\n",
        "    shutil.move(zip_file_path_local, backup_drive_folder)\n",
        "\n",
        "    print(f\"\\n✅ [성공] 로컬 DB를 Google Drive에 백업 완료!\")\n",
        "    print(f\"   - 백업 파일 위치: {zip_file_path_drive}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ [에러] 백업 중 오류가 발생했습니다: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k8SfWmE5WuY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import chromadb\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import traceback\n",
        "from IPython.display import display, HTML\n",
        "import stat # 파일 권한 변경을 위한 라이브러리\n",
        "\n",
        "# ======================================================================\n",
        "# 1. 환경 설정 및 DB 복원 (권한 문제 해결)\n",
        "# ======================================================================\n",
        "print(\"1. 환경 설정 및 DB 복원을 시작합니다...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except Exception: pass\n",
        "!pip install pandas chromadb sentence-transformers gradio -q\n",
        "\n",
        "backup_zip_path = '/content/drive/MyDrive/ai_poc_project/db_backup/db_backup_20250929.zip'\n",
        "local_db_path = '/content/db_data'\n",
        "\n",
        "try:\n",
        "    if os.path.exists(local_db_path):\n",
        "        shutil.rmtree(local_db_path)\n",
        "    shutil.unpack_archive(backup_zip_path, local_db_path, 'zip')\n",
        "    print(f\"✅ [성공] DB 복원 완료! (경로: {local_db_path})\")\n",
        "\n",
        "    # ======== 수정된 부분 ========\n",
        "    # 복원된 모든 파일 및 폴더에 쓰기 권한(chmod 755) 부여\n",
        "    print(\"   - 복원된 DB 파일에 쓰기 권한을 부여합니다...\")\n",
        "    for root, dirs, files in os.walk(local_db_path):\n",
        "        for d in dirs:\n",
        "            os.chmod(os.path.join(root, d), stat.S_IRWXU | stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)\n",
        "        for f in files:\n",
        "            os.chmod(os.path.join(root, f), stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)\n",
        "    print(\"   - 권한 부여 완료.\")\n",
        "    # ============================\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ [에러] DB 복원/권한 설정 중 오류가 발생했습니다: {e}\")\n",
        "    raise SystemExit(\"DB 복원 실패.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 2. 분석 함수 정의\n",
        "# ======================================================================\n",
        "\n",
        "def calculate_cosine_similarity(query_embedding, result_embeddings):\n",
        "    query_vector = np.array(query_embedding)\n",
        "    result_vectors = np.array(result_embeddings)\n",
        "    query_norm = np.linalg.norm(query_vector)\n",
        "    if query_norm == 0: return [0.0] * len(result_vectors)\n",
        "    query_vector /= query_norm\n",
        "    result_norms = np.linalg.norm(result_vectors, axis=1, keepdims=True)\n",
        "    result_vectors /= np.where(result_norms == 0, 1, result_norms)\n",
        "    return np.dot(result_vectors, query_vector).tolist()\n",
        "\n",
        "### PoC #2: 품목별 유사 상품 추천 함수 ###\n",
        "def analyze_item_similarity(target_item_identifier: str):\n",
        "    try:\n",
        "        if not target_item_identifier:\n",
        "            return \"분석할 품목을 선택 또는 입력해주세요.\", \"\"\n",
        "\n",
        "        if ':' in target_item_identifier:\n",
        "            target_item_code = str(target_item_identifier.split(':')[0].strip())\n",
        "        else:\n",
        "            target_item_code = str(target_item_identifier.strip())\n",
        "\n",
        "        selected_item_series = enriched_df[enriched_df['품목코드'].astype(str) == target_item_code].iloc[0]\n",
        "\n",
        "        target_embedding = item_collection.get(ids=[target_item_code], include=['embeddings'])['embeddings'][0]\n",
        "        similar_items_result = item_collection.query(query_embeddings=[target_embedding], n_results=15, include=['metadatas', 'embeddings'])\n",
        "\n",
        "        result_embeddings = similar_items_result['embeddings'][0]\n",
        "        result_metadatas = similar_items_result['metadatas'][0]\n",
        "        similarity_scores = calculate_cosine_similarity(target_embedding, result_embeddings)\n",
        "\n",
        "        qualified_items = []\n",
        "        for score, meta in zip(similarity_scores, result_metadatas):\n",
        "            if str(meta['품목코드']) != target_item_code and score >= 0.75:\n",
        "                item_details = enriched_df[enriched_df['품목코드'] == meta['품목코드']].iloc[0].to_dict()\n",
        "                item_details['유사도'] = score\n",
        "                qualified_items.append(item_details)\n",
        "\n",
        "        recommended_df = pd.DataFrame(qualified_items).sort_values(by='유사도', ascending=False).head(5)\n",
        "\n",
        "        selected_item_html = f\"\"\"\n",
        "        <h4>선택된 품목 정보</h4>\n",
        "        <div style=\"display:flex; align-items:center; border: 1px solid #E0E0E0; padding: 10px; border-radius: 5px;\">\n",
        "            <img src=\"{selected_item_series.get('이미지_URL', '')}\" width=\"100\" style=\"margin-right:20px; border: 1px solid #DDD;\">\n",
        "            <div>\n",
        "                <b>{selected_item_series.get('한글품명', '')} ({selected_item_series.get('품목코드', '')})</b><br>\n",
        "                <b>카테고리:</b> {selected_item_series.get('네이버_카테고리1', '')} > {selected_item_series.get('네이버_카테고리2', '')}<br>\n",
        "                <b>규격:</b> {selected_item_series.get('규격', '')}\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        recommend_html = \"<h4>AI 추천 유사 품목</h4>\"\n",
        "        if recommended_df.empty:\n",
        "            recommend_html += \"<p>기준점수(0.75)를 넘는 유사 품목을 찾을 수 없습니다.</p>\"\n",
        "        else:\n",
        "            for _, row in recommended_df.iterrows():\n",
        "                recommend_html += f\"\"\"\n",
        "                <div style=\"display:flex; align-items:center; border: 1px solid #4CAF50; padding: 10px; border-radius: 5px; margin-bottom:10px;\">\n",
        "                    <img src=\"{row.get('이미지_URL', '')}\" width=\"100\" style=\"margin-right:20px; border: 1px solid #DDD;\">\n",
        "                    <div>\n",
        "                        <b>{row.get('한글품명', '')} ({row.get('품목코드', '')}) - <span style=\"color:#4CAF50; font-weight:bold;\">유사도: {row.get('유사도', 0):.2f}</span></b><br>\n",
        "                        <b>카테고리:</b> {row.get('네이버_카테고리1', '')} > {row.get('네이버_카테고리2', '')}<br>\n",
        "                        <b>규격:</b> {row.get('규격', '')}\n",
        "                    </div>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "        return selected_item_html, recommend_html\n",
        "\n",
        "    except IndexError:\n",
        "        return f\"<h4>오류</h4><p>입력하신 품목코드 '{target_item_code}'를 찾을 수 없습니다.</p>\", \"\"\n",
        "    except Exception as e:\n",
        "        return f\"<h4>오류 발생</h4><p>{traceback.format_exc()}</p>\", \"\"\n",
        "\n",
        "### PoC #1: 유사 부서 상품 추천 함수 ###\n",
        "def analyze_team_preferences(target_team: str):\n",
        "    try:\n",
        "        team_sales = orders_df[orders_df['주문자소속팀'] == target_team]\n",
        "        if team_sales.empty:\n",
        "            return f\"[{target_team}]의 판매 데이터가 없습니다.\", \"분석할 유사 부서를 찾을 수 없습니다.\"\n",
        "\n",
        "        top_items = team_sales.groupby('품목코드')['수량'].sum().reset_index()\n",
        "        top_items['품명'] = top_items['품목코드'].map(item_code_to_name_poc1)\n",
        "        top_items = top_items.sort_values(by='수량', ascending=False).head(5)\n",
        "\n",
        "        ranking_text = f\"**[{target_team}] 품목 판매 랭킹 (Top 5)**\\n\"\n",
        "        for _, row in top_items.iterrows():\n",
        "            ranking_text += f\"- {row['품명']} ({row['수량']}개)\\n\"\n",
        "\n",
        "        target_embedding_result = team_collection.get(ids=[target_team], include=['embeddings'])\n",
        "        if len(target_embedding_result['embeddings']) == 0:\n",
        "            return ranking_text, \"해당 부서의 임베딩이 존재하지 않습니다.\"\n",
        "\n",
        "        target_embedding = np.array(target_embedding_result['embeddings'][0])\n",
        "        similar_teams_result = team_collection.query(query_embeddings=[target_embedding.tolist()], n_results=15, include=['documents', 'embeddings'])\n",
        "\n",
        "        results = similar_teams_result['documents'][0]\n",
        "        result_embeddings = similar_teams_result['embeddings'][0]\n",
        "\n",
        "        similarity_scores = calculate_cosine_similarity(target_embedding.tolist(), result_embeddings)\n",
        "        scored_results = sorted(zip(similarity_scores, results), key=lambda x: x[0], reverse=True)\n",
        "        similar_teams = [doc for score, doc in scored_results if doc != target_team][:3]\n",
        "\n",
        "        if not similar_teams:\n",
        "            similar_text = \"유사 부서를 찾을 수 없습니다.\"\n",
        "        else:\n",
        "            similar_teams_df = orders_df[orders_df['주문자소속팀'].isin(similar_teams)]\n",
        "            if similar_teams_df.empty:\n",
        "                similar_text = f\"유사 부서: {', '.join(similar_teams)}\\n\\n유사 부서들의 판매 데이터가 충분하지 않습니다.\"\n",
        "            else:\n",
        "                similar_top_items = similar_teams_df.groupby('품목코드')['수량'].sum().reset_index()\n",
        "                similar_top_items['품명'] = similar_top_items['품목코드'].map(item_code_to_name_poc1)\n",
        "                similar_top_items = similar_top_items.sort_values(by='수량', ascending=False).head(5)\n",
        "                similar_text = f\"**[{target_team}]과 유사한 부서들의 선호 품목 (Top 5)**\\n\"\n",
        "                similar_text += f\"(유사 부서: {', '.join(similar_teams)})\\n\"\n",
        "                for _, row in similar_top_items.iterrows():\n",
        "                    similar_text += f\"- {row['품명']} ({row['수량']}개)\\n\"\n",
        "    except Exception as e:\n",
        "        ranking_text, similar_text = \"오류 발생\", f\"분석 중 오류 발생:\\n{traceback.format_exc()}\"\n",
        "    return ranking_text, similar_text\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 3. Gradio UI 통합 구현 (v3.1)\n",
        "# ======================================================================\n",
        "print(\"\\n3. Gradio UI (v3.1)를 실행합니다...\")\n",
        "\n",
        "teams_list = sorted(orders_df['주문자소속팀'].unique().tolist())\n",
        "categories_list = sorted(enriched_df['네이버_카테고리1'].dropna().unique().tolist())\n",
        "enriched_df['item_display'] = enriched_df['품목코드'].astype(str) + \" : \" + enriched_df['한글품명']\n",
        "\n",
        "def update_item_dropdown(category):\n",
        "    if not category:\n",
        "        return gr.Dropdown(choices=[], label=\"2. 품목을 선택하세요\")\n",
        "    filtered_df = enriched_df[enriched_df['네이버_카테고리1'] == category]\n",
        "    choices = sorted(filtered_df['item_display'].dropna().unique().tolist())\n",
        "    return gr.Dropdown(choices=choices, label=\"2. 품목을 선택하세요\", value=choices[0] if choices else None)\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# 🤖 IMK Plus AI PoC Project\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # ---------- PoC #1 UI ----------\n",
        "        with gr.TabItem(\"PoC #1: 유사 부서 상품 추천\"):\n",
        "            gr.Markdown(\"특정 부서를 선택하면, 해당 부서의 판매 랭킹과 AI가 추천하는 유사 부서의 선호 품목을 분석합니다.\")\n",
        "            with gr.Row():\n",
        "                team_dropdown = gr.Dropdown(choices=teams_list, label=\"분석할 부서를 선택하세요\")\n",
        "                poc1_button = gr.Button(\"분석 실행\", variant=\"primary\")\n",
        "            with gr.Row():\n",
        "                ranking_output = gr.Textbox(label=\"선택한 부서의 품목 판매 랭킹\", lines=7)\n",
        "                similar_output = gr.Textbox(label=\"유사 부서들의 선호 품목\", lines=7)\n",
        "            poc1_button.click(fn=analyze_team_preferences, inputs=team_dropdown, outputs=[ranking_output, similar_output])\n",
        "\n",
        "        # ---------- PoC #2 UI ----------\n",
        "        with gr.TabItem(\"PoC #2: 품목별 유사 상품 추천\"):\n",
        "            gr.Markdown(\"AI가 **유사도 0.75 이상**인 품목 중 가장 유사한 **최대 5개**를 추천합니다.\")\n",
        "\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"카테고리로 찾기\"):\n",
        "                    with gr.Row():\n",
        "                        category_dd = gr.Dropdown(choices=categories_list, label=\"1. 대분류 카테고리를 선택하세요\")\n",
        "                        item_dd = gr.Dropdown(label=\"2. 품목을 선택하세요 (카테고리 먼저 선택)\", interactive=True)\n",
        "                    cat_button = gr.Button(\"유사 품목 찾기\", variant=\"primary\")\n",
        "\n",
        "                with gr.TabItem(\"직접 검색하기\"):\n",
        "                    item_code_tb = gr.Textbox(label=\"품목코드를 직접 입력하세요\", placeholder=\"예: 1419128400\")\n",
        "                    code_button = gr.Button(\"유사 품목 찾기\", variant=\"primary\")\n",
        "\n",
        "            gr.HTML(\"<hr style='margin: 20px 0;'>\")\n",
        "            selected_item_output = gr.HTML()\n",
        "            similar_items_output = gr.HTML()\n",
        "\n",
        "            category_dd.change(fn=update_item_dropdown, inputs=category_dd, outputs=item_dd)\n",
        "            cat_button.click(fn=analyze_item_similarity, inputs=item_dd, outputs=[selected_item_output, similar_items_output])\n",
        "            code_button.click(fn=analyze_item_similarity, inputs=item_code_tb, outputs=[selected_item_output, similar_items_output])\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "부서정보 없어서 에러 발생 일단 주문정보/딜리버리 정보 업로드 하자\n"
      ],
      "metadata": {
        "id": "O4Q2Ho46KNuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "# ======================================================================\n",
        "# 0. 환경 설정 및 데이터 로드\n",
        "# ======================================================================\n",
        "print(\"환경 설정 및 데이터 로드를 시작합니다...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except Exception: pass\n",
        "\n",
        "# --- 파일 경로 설정 ---\n",
        "DRIVE_PATH = '/content/drive/MyDrive/ai_poc_project/'\n",
        "ORIGINAL_DATA_PATH = os.path.join(DRIVE_PATH, 'original_data')\n",
        "ZIPCODE_DATA_PATH = os.path.join(DRIVE_PATH, 'zipcode_data')\n",
        "\n",
        "# --- 데이터 로드 ---\n",
        "try:\n",
        "    delivery_df = pd.read_csv(os.path.join(ORIGINAL_DATA_PATH, 'delivery_data_cleaned.txt'), sep='\\t')\n",
        "    print(f\"✅ 배송 데이터 로드 완료: {len(delivery_df)}건\")\n",
        "\n",
        "    zipcode_db_df = pd.read_csv(os.path.join(ZIPCODE_DATA_PATH, 'all_zipcodes.csv'))\n",
        "    print(f\"✅ 우편번호 DB 로드 완료: {len(zipcode_db_df)}건\")\n",
        "\n",
        "    # ======== 수정된 부분 ========\n",
        "    # 두 데이터프레임의 '우편번호' 컬럼을 모두 '문자열' 타입으로 통일\n",
        "    delivery_df['우편번호'] = delivery_df['우편번호'].astype(str)\n",
        "    zipcode_db_df['우편번호'] = zipcode_db_df['우편번호'].astype(str)\n",
        "    print(\"   - '우편번호' 컬럼 타입 통일 완료.\")\n",
        "    # ============================\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ 데이터 로드 중 오류가 발생했습니다: {e}\")\n",
        "    raise SystemExit(\"데이터 로드 실패.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 1. 데이터 전처리\n",
        "# ======================================================================\n",
        "print(\"\\n1. 데이터 전처리를 시작합니다...\")\n",
        "\n",
        "try:\n",
        "    # --- 1-1. '리드 타임' 계산 ---\n",
        "    delivery_df['운송지시일자'] = pd.to_datetime(delivery_df['운송지시일자'], format='%Y%m%d', errors='coerce')\n",
        "    delivery_df['운송완료일자'] = pd.to_datetime(delivery_df['운송완료일자'], format='%Y%m%d', errors='coerce')\n",
        "    delivery_df['리드타임'] = (delivery_df['운송완료일자'] - delivery_df['운송지시일자']).dt.days\n",
        "    print(\"   - '리드타임' 컬럼 생성 완료.\")\n",
        "\n",
        "    # --- 1-2. '도시' 정보 통합 매핑 ---\n",
        "    zipcode_map = zipcode_db_df[['우편번호', '시도']].rename(columns={'시도': '도시'})\n",
        "    preprocessed_df = pd.merge(delivery_df, zipcode_map, on='우편번호', how='left')\n",
        "    print(\"   - '우편번호'를 기준으로 '도시' 정보 전체 매핑 완료.\")\n",
        "\n",
        "    # --- 1-3. 컬럼명 변경 및 최종 정제 ---\n",
        "    preprocessed_df.rename(columns={'납품물류유형': '배송유형'}, inplace=True)\n",
        "    final_cols = ['품목코드', '배송유형', '도시', '리드타임']\n",
        "    final_df = preprocessed_df[final_cols].copy()\n",
        "\n",
        "    final_df.dropna(inplace=True)\n",
        "    final_df = final_df[final_df['리드타임'] >= 0]\n",
        "    print(\"   - 최종 데이터 정제 완료.\")\n",
        "\n",
        "    # ======================================================================\n",
        "    # 2. 결과 확인\n",
        "    # ======================================================================\n",
        "    print(\"\\n✨ [성공] AI 학습용 데이터 준비가 완료되었습니다.\")\n",
        "    print(f\"   - 최종 분석용 데이터: {len(final_df)}건\")\n",
        "    print(\"\\n[최종 데이터 샘플 (상위 5개)]\")\n",
        "    display(final_df.head())\n",
        "\n",
        "    print(\"\\n[도시별 데이터 건수]\")\n",
        "    print(final_df['도시'].value_counts())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ [에러] 전처리 중 오류가 발생했습니다: {e}\")\n",
        "    print(traceback.format_exc())"
      ],
      "metadata": {
        "id": "Hy86Js8NLZ5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "주문 데이터 적치"
      ],
      "metadata": {
        "id": "lLMesVhhNqQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import shutil\n",
        "import stat\n",
        "\n",
        "# --- 0. 환경 설정 및 데이터 로드 ---\n",
        "print(\"환경 설정 및 데이터 로드를 시작합니다...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except Exception: pass\n",
        "!pip install pandas chromadb sentence-transformers -q\n",
        "\n",
        "local_db_path = '/content/db_data'\n",
        "file_path = '/content/drive/MyDrive/ai_poc_project/original_data/orders_final.txt'\n",
        "\n",
        "try:\n",
        "    # --- 1. DB 복원 ---\n",
        "    backup_zip_path = f\"/content/drive/MyDrive/ai_poc_project/db_backup/db_backup_{pd.Timestamp.now().strftime('%Y%m%d')}.zip\"\n",
        "    if os.path.exists(local_db_path): shutil.rmtree(local_db_path)\n",
        "    shutil.unpack_archive(backup_zip_path, local_db_path, 'zip')\n",
        "    print(\"✅ 기존 DB 복원 완료.\")\n",
        "    for root, dirs, files in os.walk(local_db_path):\n",
        "        for d in dirs: os.chmod(os.path.join(root, d), stat.S_IRWXU | stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)\n",
        "        for f in files: os.chmod(os.path.join(root, f), stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)\n",
        "\n",
        "    # --- 2. 신규 주문 데이터 로드 ---\n",
        "    orders_df = pd.read_csv(file_path, sep='\\t')\n",
        "    print(f\"✅ 신규 주문 데이터 로드 완료: {len(orders_df)}건\")\n",
        "\n",
        "    # ======== 수정된 부분 (강화된 정제 로직) ========\n",
        "    # --- 2-1. 주문 데이터 정제 ---\n",
        "    # '주문수량'을 숫자로 변환 (숫자가 아니면 강제로 NaT/NaN 값으로 변경)\n",
        "    orders_df['주문수량'] = pd.to_numeric(orders_df['주문수량'], errors='coerce')\n",
        "    # '주문수량'에 유효한 숫자가 없는 모든 행(row) 제거\n",
        "    orders_df.dropna(subset=['주문수량'], inplace=True)\n",
        "    # '주문수량'을 정수(integer) 타입으로 최종 변환\n",
        "    orders_df['주문수량'] = orders_df['주문수량'].astype(int)\n",
        "    print(f\"   - '주문수량' 정제 완료. (유효 데이터: {len(orders_df)}건)\")\n",
        "    # ===============================================\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ [에러] 초기 설정 중 오류가 발생했습니다: {e}\")\n",
        "    raise SystemExit(\"초기 설정 실패.\")\n",
        "\n",
        "# --- 3. 부서별 구매 패턴으로 \"AI 학습 자료\" 재구성 ---\n",
        "print(\"\\n부서별 구매 패턴을 분석하여 학습 자료를 구성합니다...\")\n",
        "team_purchase_list = orders_df.loc[orders_df.index.repeat(orders_df['주문수량'])]\n",
        "team_embedding_text = team_purchase_list.groupby('주문자부서명')['품목코드'].apply(lambda x: ' '.join(x.astype(str)))\n",
        "team_df = team_embedding_text.reset_index()\n",
        "print(\"✅ 부서별 학습 자료 생성 완료.\")\n",
        "\n",
        "# --- 4. 부서(팀) 벡터 생성 및 \"로컬 DB\" 업데이트 ---\n",
        "print(\"\\n'team_embeddings' 컬렉션을 새로 구축합니다...\")\n",
        "client = chromadb.PersistentClient(path=local_db_path)\n",
        "model_name = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "team_embeddings = model.encode(team_df['품목코드'].tolist(), show_progress_bar=True)\n",
        "\n",
        "try:\n",
        "    client.delete_collection(name=\"team_embeddings\")\n",
        "    print(\"   - 기존 team_embeddings 컬렉션을 삭제했습니다.\")\n",
        "except Exception:\n",
        "    print(\"   - 신규 team_embeddings 컬렉션을 생성합니다.\")\n",
        "\n",
        "team_collection = client.get_or_create_collection(\"team_embeddings\")\n",
        "team_collection.add(\n",
        "    embeddings=team_embeddings.tolist(),\n",
        "    documents=team_df['주문자부서명'].tolist(),\n",
        "    ids=team_df['주문자부서명'].tolist()\n",
        ")\n",
        "\n",
        "print(\"\\n✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨\")\n",
        "print(f\"✅ [성공] 로컬 DB에 'team_embeddings' 재구축 완료!\")\n",
        "print(f\"   - 최종 저장된 부서 데이터 수: {team_collection.count()}개\")\n",
        "print(\"✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨\")"
      ],
      "metadata": {
        "id": "jW9bRUYZNuSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# --- 경로 설정 ---\n",
        "local_db_path = '/content/db_data'\n",
        "backup_drive_folder = '/content/drive/MyDrive/ai_poc_project/db_backup'\n",
        "# --- 경로 설정 끝 ---\n",
        "\n",
        "print(f\"\\n업데이트된 로컬 DB를 Google Drive에 백업합니다...\")\n",
        "\n",
        "try:\n",
        "    os.makedirs(backup_drive_folder, exist_ok=True)\n",
        "    backup_filename = f\"db_backup_{pd.Timestamp.now().strftime('%Y%m%d')}\"\n",
        "    zip_file_path_local = f\"{backup_filename}.zip\"\n",
        "    zip_file_path_drive = os.path.join(backup_drive_folder, zip_file_path_local)\n",
        "\n",
        "    if os.path.exists(zip_file_path_drive):\n",
        "        os.remove(zip_file_path_drive)\n",
        "        print(f\"   - 기존 백업 파일({zip_file_path_drive})을 삭제했습니다.\")\n",
        "\n",
        "    shutil.make_archive(backup_filename, 'zip', local_db_path)\n",
        "    shutil.move(zip_file_path_local, backup_drive_folder)\n",
        "\n",
        "    print(f\"\\n✅ [성공] 업데이트된 DB 백업 완료!\")\n",
        "    print(f\"   - 백업 파일 위치: {zip_file_path_drive}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ [에러] 백업 중 오류가 발생했습니다: {e}\")"
      ],
      "metadata": {
        "id": "Az8sEhSISBRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import joblib\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "# ======================================================================\n",
        "# 0. 데이터 전처리 (이전 단계 요약)\n",
        "# ======================================================================\n",
        "print(\"0. AI 학습용 데이터를 준비합니다...\")\n",
        "try:\n",
        "    # Google Drive 연결\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # 경로 설정\n",
        "    DRIVE_PATH = '/content/drive/MyDrive/ai_poc_project/'\n",
        "    ORIGINAL_DATA_PATH = os.path.join(DRIVE_PATH, 'original_data')\n",
        "    ZIPCODE_DATA_PATH = os.path.join(DRIVE_PATH, 'zipcode_data')\n",
        "\n",
        "    # 데이터 로드\n",
        "    delivery_df = pd.read_csv(os.path.join(ORIGINAL_DATA_PATH, 'delivery_data_cleaned.txt'), sep='\\t')\n",
        "    zipcode_db_df = pd.read_csv(os.path.join(ZIPCODE_DATA_PATH, 'all_zipcodes.csv'))\n",
        "\n",
        "    # 타입 통일\n",
        "    delivery_df['우편번호'] = delivery_df['우편번호'].astype(str)\n",
        "    zipcode_db_df['우편번호'] = zipcode_db_df['우편번호'].astype(str)\n",
        "\n",
        "    # 전처리\n",
        "    delivery_df['운송지시일자'] = pd.to_datetime(delivery_df['운송지시일자'], format='%Y%m%d', errors='coerce')\n",
        "    delivery_df['운송완료일자'] = pd.to_datetime(delivery_df['운송완료일자'], format='%Y%m%d', errors='coerce')\n",
        "    delivery_df['리드타임'] = (delivery_df['운송완료일자'] - delivery_df['운송지시일자']).dt.days\n",
        "    zipcode_map = zipcode_db_df[['우편번호', '시도']].rename(columns={'시도': '도시'})\n",
        "    preprocessed_df = pd.merge(delivery_df, zipcode_map, on='우편번호', how='left')\n",
        "    preprocessed_df.rename(columns={'납품물류유형': '배송유형'}, inplace=True)\n",
        "    final_df = preprocessed_df[['품목코드', '배송유형', '도시', '리드타임']].copy()\n",
        "    final_df.dropna(inplace=True)\n",
        "    final_df = final_df[final_df['리드타임'] >= 0]\n",
        "    print(f\"✅ 데이터 준비 완료. (총 {len(final_df)}건)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ 데이터 준비 중 오류: {e}\")\n",
        "    raise SystemExit(\"실패\")\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 1. AI 학습을 위한 데이터 변환 (One-Hot Encoding)\n",
        "# ======================================================================\n",
        "print(\"\\n1. 전체 데이터를 AI가 학습할 수 있도록 변환합니다...\")\n",
        "features_df = pd.get_dummies(final_df, columns=['품목코드', '배송유형', '도시'])\n",
        "X = features_df.drop('리드타임', axis=1)\n",
        "y = features_df['리드타임']\n",
        "print(\"✅ 데이터 변환 완료.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 2. 학습용 / 테스트용 데이터 분리\n",
        "# ======================================================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"   - 학습용 데이터 {len(X_train)}건, 테스트용 데이터 {len(X_test)}건 분리 완료.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 3. 랜덤 포레스트 모델 학습\n",
        "# ======================================================================\n",
        "print(\"\\n2. 랜덤 포레스트 모델 학습을 시작합니다... (Colab Pro 고용량 RAM으로 실행 중)\")\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=20, min_samples_leaf=10)\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"✅ 모델 학습 완료!\")\n",
        "\n",
        "# ======================================================================\n",
        "# 4. 모델 성능 검증 (MAE 계산)\n",
        "# ======================================================================\n",
        "predictions = rf_model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "print(\"\\n✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨\")\n",
        "print(\"✅ [성공] 모델 성능 검증 완료!\")\n",
        "print(f\"   - 예측 평균 절대 오차 (MAE): {mae:.2f} 일\")\n",
        "print(\"✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨\")\n",
        "\n",
        "# ======================================================================\n",
        "# 5. 학습된 모델과 컬럼 정보 저장\n",
        "# ======================================================================\n",
        "MODEL_PATH = os.path.join(DRIVE_PATH, 'models')\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "model_filename = os.path.join(MODEL_PATH, 'lead_time_predictor.joblib')\n",
        "joblib.dump(rf_model, model_filename)\n",
        "print(f\"\\n✅ 학습된 AI 모델이 아래 경로에 저장되었습니다:\\n   - {model_filename}\")\n",
        "columns_filename = os.path.join(MODEL_PATH, 'lead_time_model_columns.joblib')\n",
        "joblib.dump(X.columns, columns_filename)\n",
        "print(f\"\\n✅ 모델 컬럼 정보가 아래 경로에 저장되었습니다:\\n   - {columns_filename}\")"
      ],
      "metadata": {
        "id": "BwAPzoFyVL5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "최종 마스터 버전 정리"
      ],
      "metadata": {
        "id": "08TJYG89Tb_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "서버 재기동 후 통합 수행 소스 아래 부터 시작 (마스터 수행코드)"
      ],
      "metadata": {
        "id": "V7TTEF_jgk63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# 1. 환경 설정 및 라이브러리 설치 (최신 버전 사용)\n",
        "# ======================================================================\n",
        "print(\"1. 전체 환경 설정을 시작합니다...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except Exception: pass\n",
        "\n",
        "# !! 중요 !! : 버전을 고정하지 않고, 호환되는 최신 라이브러리를 설치합니다.\n",
        "print(\"\\n필수 라이브러리를 최신 버전으로 설치합니다...\")\n",
        "!pip install pandas chromadb sentence-transformers gradio scikit-learn lightgbm --upgrade -q\n",
        "print(\"✅ 라이브러리 설치 완료!\")\n",
        "\n",
        "# !! 중요 !! : 라이브러리 설치가 끝난 후, 모든 import를 시작합니다.\n",
        "import pandas as pd\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import traceback\n",
        "from IPython.display import display, HTML\n",
        "import stat\n",
        "import joblib\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "import lightgbm as lgb\n",
        "\n",
        "# ======================================================================\n",
        "# 2. 데이터 로드, DB 구축 및 모델 준비\n",
        "# ======================================================================\n",
        "print(\"\\n2. 데이터 로드 및 AI 모델 준비를 시작합니다...\")\n",
        "\n",
        "# --- 경로 설정 ---\n",
        "DRIVE_PATH = '/content/drive/MyDrive/ai_poc_project/'\n",
        "ORIGINAL_DATA_PATH = os.path.join(DRIVE_PATH, 'original_data')\n",
        "MODEL_PATH = os.path.join(DRIVE_PATH, 'models')\n",
        "ZIPCODE_DATA_PATH = os.path.join(DRIVE_PATH, 'zipcode_data')\n",
        "\n",
        "# --- 필요 데이터 로드 ---\n",
        "try:\n",
        "    print(\"\\n원본 데이터들을 로드합니다...\")\n",
        "    enriched_df = pd.read_csv(os.path.join(ORIGINAL_DATA_PATH, 'item_data_enriched.csv'))\n",
        "    orders_df_raw = pd.read_csv(os.path.join(ORIGINAL_DATA_PATH, 'orders_final.txt'), sep='\\t')\n",
        "    delivery_df_raw = pd.read_csv(os.path.join(ORIGINAL_DATA_PATH, 'delivery_data_cleaned.txt'), sep='\\t')\n",
        "    zipcode_db_df = pd.read_csv(os.path.join(ZIPCODE_DATA_PATH, 'all_zipcodes.csv'))\n",
        "    print(\"✅ 모든 소스 데이터 로드 완료.\")\n",
        "\n",
        "    # ======== 수정된 부분: '품목코드' 타입을 '문자열'로 강제 통일 ========\n",
        "    enriched_df['품목코드'] = enriched_df['품목코드'].astype(str)\n",
        "    orders_df_raw['품목코드'] = orders_df_raw['품목코드'].astype(str)\n",
        "    delivery_df_raw['품목코드'] = delivery_df_raw['품목코드'].astype(str)\n",
        "    print(\" - 모든 '품목코드' 타입을 문자열(str)로 통일했습니다.\")\n",
        "    # ==========================================================\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ 데이터 로드 중 오류 발생: {e}\")\n",
        "\n",
        "# --- PoC #1 데이터 정합성 필터링 및 그룹화 ---\n",
        "print(\" - PoC #1용 데이터를 처리합니다...\")\n",
        "try:\n",
        "    valid_item_codes = set(enriched_df['품목코드'].unique())\n",
        "    orders_df = orders_df_raw[orders_df_raw['품목코드'].isin(valid_item_codes)].copy()\n",
        "    department_rules = {\n",
        "        '연구/개발': ['개발', 'Lab', 'R&D', '연구', '설계', 'S/W', 'AI', 'Solution', 'Platform', 'Tech', '기술', '선행', '메카', 'Digital'], '영업/마케팅': ['영업', '마케팅', 'Retail', 'Sales', 'PM', 'Biz'],\n",
        "        '품질/검증': ['품질', 'QA', 'QC', '검사', '신뢰성', '보증', 'Validation', 'Compliance'], '인사/총무': ['인사', '총무', '채용', '교육', 'HR', '지원', '상생', 'ER'],\n",
        "        '생산/공정': ['생산', '제조', 'FAB', '공정', 'PJT', 'P/J', '라인', 'Line', 'CELL', 'Plant', '정제', '충전', '전처리'], '구매/자재': ['구매', '자재', 'Spare'], '물류': ['물류', 'SCM'],\n",
        "        '재무/회계': ['재무', '경리', '회계', '자금', 'IR', '결산'], '기획/전략': ['기획', '전략', '혁신'], '디자인': ['디자인', 'UX', 'UI'], 'CS/서비스': ['CS', '서비스', '홈케어'],\n",
        "        '안전/환경': ['안전', '환경', 'EHS', '보건', '방재'], '지점/지역': ['지점', '지국', '총국', '센터'], '설비/인프라': ['설비', 'GAS', 'FT', '공무', '소방', 'Infra', '전기']\n",
        "    }\n",
        "    company_keywords = ['(주)', '㈜', '코리아', '삼성', 'SK', 'CJ', '현대', 'LG', '롯데']\n",
        "    def group_team_name(team_name):\n",
        "        if not isinstance(team_name, str): return '기타'\n",
        "        team_name_lower = team_name.lower()\n",
        "        for group, keywords in department_rules.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword.lower() in team_name_lower: return group\n",
        "        for keyword in company_keywords:\n",
        "            if keyword in team_name:\n",
        "                clean_name = team_name.split('(')[0].strip(); return clean_name if clean_name else team_name\n",
        "        return '기타' # Revert change to '기 기타'\n",
        "    orders_df['부서그룹'] = orders_df['주문자부서명'].apply(group_team_name)\n",
        "    orders_df['주문수량'] = pd.to_numeric(orders_df['주문수량'], errors='coerce')\n",
        "    orders_df.dropna(subset=['주문수량'], inplace=True)\n",
        "    orders_df['주문수량'] = orders_df['주문수량'].astype(int)\n",
        "\n",
        "    poc1_data_ready = True # Add flag\n",
        "    print(\" - ✅ PoC #1용 주문 데이터 처리 완료.\")\n",
        "except Exception as e:\n",
        "    poc1_data_ready = False # Set flag to False on error\n",
        "    print(f\" - ⚠️ PoC #1 데이터 처리 실패: {e}\")\n",
        "\n",
        "\n",
        "# --- 인메모리(In-Memory) DB 구축 ---\n",
        "print(\"\\n - 인메모리 DB 구축을 시작합니다... (약 5-7분 소요)\")\n",
        "client = chromadb.Client()\n",
        "model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
        "# PoC #2 DB 생성\n",
        "cols_to_embed = ['품목코드', '한글품명', '영문품명', '규격', '모델명', '제조원명', '네이버_카테고리1', '네이버_카테고리2', '네이버_카테고리3', '네이버_카테고리4', '브랜드', '최저가']\n",
        "enriched_df['embedding_text'] = enriched_df[cols_to_embed].fillna('').astype(str).agg(' '.join, axis=1)\n",
        "item_embeddings = model.encode(enriched_df['embedding_text'].tolist(), show_progress_bar=True)\n",
        "item_collection = client.get_or_create_collection(\"item_embeddings\")\n",
        "item_collection.add(embeddings=item_embeddings.tolist(), metadatas=[{\"품목코드\": code} for code in enriched_df['품목코드'].tolist()], documents=enriched_df['embedding_text'].tolist(), ids=[str(code) for code in enriched_df['품목코드'].tolist()])\n",
        "# PoC #1 DB 생성 (Only if data was processed successfully)\n",
        "if poc1_data_ready: # Check flag\n",
        "    print(\"   - PoC #1 'team_embeddings' 생성 중...\")\n",
        "    team_purchase_list = orders_df.loc[orders_df.index.repeat(orders_df['주문수량'])]\n",
        "    team_embedding_text = team_purchase_list.groupby('부서그룹')['품목코드'].apply(lambda x: ' '.join(x.astype(str)))\n",
        "    team_df = team_embedding_text.reset_index()\n",
        "    team_embeddings = model.encode(team_df['품목코드'].tolist(), show_progress_bar=True)\n",
        "    team_collection = client.get_or_create_collection(\"team_embeddings\")\n",
        "    team_collection.add(embeddings=team_embeddings.tolist(), documents=team_df['부서그룹'].tolist(), ids=team_df['부서그룹'].tolist())\n",
        "    print(f\" - ✅ DB 구축 완료!\")\n",
        "else:\n",
        "    print(\" - ⚠️ PoC #1 데이터 처리 실패로 'team_embeddings' DB 구축을 건너뛰었습니다.\")\n",
        "\n",
        "\n",
        "# --- PoC#3 AI 모델 로드 및 데이터 전처리 ---\n",
        "# --- AI 모델 로드 ---\n",
        "try:\n",
        "    print(\"\\nAI 모델을 로드합니다...\")\n",
        "    # 우리가 마지막으로 성공시킨 LightGBM 모델과 그에 맞는 Hasher를 로드합니다.\n",
        "    lead_time_model = joblib.load(os.path.join(MODEL_PATH, 'lead_time_predictor_lgbm.joblib'))\n",
        "    hasher = joblib.load(os.path.join(MODEL_PATH, 'lead_time_hasher_final.joblib'))\n",
        "    print(\"✅ PoC #3 예측 모델(LightGBM) 및 Hasher 로드 완료.\")\n",
        "except Exception as e:\n",
        "    lead_time_model, hasher = None, None\n",
        "    print(f\"⚠️ PoC #3 모델 로드 실패: {e}\")\n",
        "# =================================================\n",
        "\n",
        "# --- PoC #3 데이터 전처리 (별도의 try 블록으로 이동) ---\n",
        "print(\"\\n - PoC #3용 배송 데이터를 전처리합니다...\")\n",
        "try:\n",
        "    delivery_df_processed = delivery_df_raw.copy()\n",
        "    delivery_df_processed['우편번호'] = delivery_df_processed['우편번호'].astype(str)\n",
        "    zipcode_db_df['우편번호'] = zipcode_db_df['우편번호'].astype(str)\n",
        "    delivery_df_processed['운송지시일자'] = pd.to_datetime(delivery_df_processed['운송지시일자'], format='%Y%m%d', errors='coerce')\n",
        "    delivery_df_processed['운송완료일자'] = pd.to_datetime(delivery_df_processed['운송완료일자'], format='%Y%m%d', errors='coerce')\n",
        "    delivery_df_processed['리드타임'] = (delivery_df_processed['운송완료일자'] - delivery_df_processed['운송지시일자']).dt.days\n",
        "    delivery_df_processed['주문_월'] = delivery_df_processed['운송지시일자'].dt.month\n",
        "    delivery_df_processed['주문_요일'] = delivery_df_processed['운송지시일자'].dt.dayofweek\n",
        "    zipcode_map = zipcode_db_df[['우편번호', '시도']].rename(columns={'시도': '도시'})\n",
        "    delivery_df_processed = pd.merge(delivery_df_processed, zipcode_map, on='우편번호', how='left')\n",
        "    delivery_df_processed.rename(columns={'납품물류유형': '배송유형'}, inplace=True)\n",
        "    delivery_df_processed.dropna(subset=['품목코드', '배송유형', '도시', '리드타임', '주문_월', '주문_요일'], inplace=True)\n",
        "    delivery_df_processed = delivery_df_processed[delivery_df_processed['리드타임'] >= 0]\n",
        "    poc3_data_ready = True # Add flag\n",
        "    print(\" - ✅ PoC #3 준비 완료.\")\n",
        "except Exception as e:\n",
        "    lead_time_model, hasher, delivery_df_processed = None, None, None\n",
        "    poc3_data_ready = False # Set flag to False on error\n",
        "    print(f\" - ⚠️ PoC #3 준비 실패: {e}.\")\n",
        "\n",
        "print(\"\\n✨ 모든 모델과 데이터가 메모리에 준비되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609,
          "referenced_widgets": [
            "08d598cdc5ce4b42b1903c96edea593f",
            "d277a2ce92b84520960d6b78ef223f97",
            "5ff5d99454b844c3876a2dc786ca8836",
            "3ad87978438a4bf2a6a8e29905346b58",
            "c9a9bd4871f2453681d3da5465ef18a1",
            "9c742b88896249678ee3dad4dddb5e51",
            "878dfd4d7f0e46438f8331ca5774825e",
            "ebfc2e8c8e524d8784b13e9b40c6bed2",
            "1642bc754d0c45a5837f63c2d031439b",
            "2935f090be714175988b1ae1a40a85c0",
            "d2d2913c9593404da50883dd6a0eefb0",
            "7c39223f8ea245ef8144a736e9fe92e8",
            "a5a305cad59d44598f8f6a308d0d433d",
            "9ff2fffa5c69436388dc98f9e13c9ef2",
            "f7d85e78d3634b88948741d39ae7f472",
            "05428539762146f882a87fd02bdb617c",
            "9377f5c94e7649d19929e759844e018b",
            "9014c49cd3a040fca029c2b92b250a18",
            "7d100a0a4a9b407a8b0b0e6c2d777e6d",
            "6c22ce4e1b204cb6b946400972a7e168",
            "e35e73cb2a334a5d96e8aa6c2a7b969d",
            "f83e710f667a42f2aa9a31b0f8c86eea"
          ]
        },
        "id": "fFwqsHwHba3R",
        "outputId": "dfecf27c-efc5-4fdf-a6a7-dce8e78fee7d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. 전체 환경 설정을 시작합니다...\n",
            "Mounted at /content/drive\n",
            "\n",
            "필수 라이브러리를 최신 버전으로 설치합니다...\n",
            "✅ 라이브러리 설치 완료!\n",
            "\n",
            "2. 데이터 로드 및 AI 모델 준비를 시작합니다...\n",
            "\n",
            "원본 데이터들을 로드합니다...\n",
            "✅ 모든 소스 데이터 로드 완료.\n",
            " - 모든 '품목코드' 타입을 문자열(str)로 통일했습니다.\n",
            " - PoC #1용 데이터를 처리합니다...\n",
            " - ✅ PoC #1용 주문 데이터 처리 완료.\n",
            "\n",
            " - 인메모리 DB 구축을 시작합니다... (약 5-7분 소요)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08d598cdc5ce4b42b1903c96edea593f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   - PoC #1 'team_embeddings' 생성 중...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c39223f8ea245ef8144a736e9fe92e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - ✅ DB 구축 완료!\n",
            "\n",
            "AI 모델을 로드합니다...\n",
            "✅ PoC #3 예측 모델(LightGBM) 및 Hasher 로드 완료.\n",
            "\n",
            " - PoC #3용 배송 데이터를 전처리합니다...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator FeatureHasher from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - ✅ PoC #3 준비 완료.\n",
            "\n",
            "✨ 모든 모델과 데이터가 메모리에 준비되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import traceback\n",
        "from IPython.display import display, HTML\n",
        "import gradio as gr\n",
        "from sentence_transformers import SentenceTransformer # 2단계에서 import했는지 확인\n",
        "\n",
        "# ======================================================================\n",
        "# 2. 모든 PoC 분석 함수 정의 (PoC #3 함수 수정)\n",
        "# ======================================================================\n",
        "print(\"2. 분석 함수 정의를 시작합니다...\")\n",
        "\n",
        "# (PoC #1, #2 함수는 이전과 동일)\n",
        "def calculate_cosine_similarity(query_embedding, result_embeddings):\n",
        "    query_vector = np.array(query_embedding); result_vectors = np.array(result_embeddings)\n",
        "    query_norm = np.linalg.norm(query_vector)\n",
        "    if query_norm == 0: return [0.0] * len(result_vectors)\n",
        "    query_vector /= query_norm\n",
        "    result_norms = np.linalg.norm(result_vectors, axis=1, keepdims=True)\n",
        "    result_vectors /= np.where(result_norms == 0, 1, result_norms)\n",
        "    return np.dot(result_vectors, query_vector).tolist()\n",
        "\n",
        "def analyze_item_similarity(target_item_identifier: str):\n",
        "    try:\n",
        "        if not target_item_identifier: return \"분석할 품목을 선택 또는 입력해주세요.\", \"\"\n",
        "        target_item_code = str(target_item_identifier.split(':')[0].strip()) if ':' in target_item_identifier else str(target_item_identifier.strip())\n",
        "        selected_item_series = enriched_df[enriched_df['품목코드'].astype(str) == target_item_code].iloc[0]\n",
        "        target_category = selected_item_series['네이버_카테고리1']\n",
        "        target_embedding = item_collection.get(ids=[target_item_code], include=['embeddings'])['embeddings'][0]\n",
        "        similar_items_result = item_collection.query(query_embeddings=[target_embedding], n_results=15, include=['metadatas', 'embeddings'])\n",
        "        result_embeddings, result_metadatas = similar_items_result['embeddings'][0], similar_items_result['metadatas'][0]\n",
        "        similarity_scores = calculate_cosine_similarity(target_embedding, result_embeddings)\n",
        "        qualified_items = []\n",
        "        for score, meta in zip(similarity_scores, result_metadatas):\n",
        "            if str(meta['품목코드']) != target_item_code and score >= 0.75:\n",
        "                item_details = enriched_df[enriched_df['품목코드'] == meta['품목코드']].iloc[0].to_dict()\n",
        "                if item_details.get('네이버_카테고리1') == target_category:\n",
        "                    item_details['유사도'] = score\n",
        "                    qualified_items.append(item_details)\n",
        "        if not qualified_items: recommended_df = pd.DataFrame()\n",
        "        else: recommended_df = pd.DataFrame(qualified_items).sort_values(by='유사도', ascending=False).head(5)\n",
        "        selected_item_html = f\"\"\"<h4>선택된 품목 정보</h4><div style=\"display:flex; align-items:center; border: 1px solid #E0E0E0; padding: 10px; border-radius: 5px;\"><img src=\"{selected_item_series.get('이미지_URL', '')}\" width=\"100\" style=\"margin-right:20px; border: 1px solid #DDD;\"><div><b>{selected_item_series.get('한글품명', '')} ({selected_item_series.get('품목코드', '')})</b><br><b>카테고리:</b> {selected_item_series.get('네이버_카테고리1', '')} > {selected_item_series.get('네이버_카테고리2', '')}<br><b>규격:</b> {selected_item_series.get('규격', '')}</div></div>\"\"\"\n",
        "        recommend_html = \"<h4>AI 추천 유사 품목</h4>\"\n",
        "        if recommended_df.empty: recommend_html += f\"<p>'{target_category}' 카테고리 내에서 기준점수(0.75)를 넘는 유사 품목을 찾을 수 없습니다.</p>\"\n",
        "        else:\n",
        "            for _, row in recommended_df.iterrows():\n",
        "                recommend_html += f\"\"\"<div style=\"display:flex; align-items:center; border: 1px solid #4CAF50; padding: 10px; border-radius: 5px; margin-bottom:10px;\"><img src=\"{row.get('이미지_URL', '')}\" width=\"100\" style=\"margin-right:20px; border: 1px solid #DDD;\"><div><b>{row.get('한글품명', '')} ({row.get('품목코드', '')}) - <span style=\"color:#4CAF50; font-weight:bold;\">유사도: {row.get('유사도', 0):.2f}</span></b><br><b>카테고리:</b> {row.get('네이버_카테고리1', '')} > {row.get('네이버_카테고리2', '')}<br><b>규격:</b> {row.get('규격', '')}</div></div>\"\"\"\n",
        "        return selected_item_html, recommend_html\n",
        "    except IndexError: return f\"<h4>오류</h4><p>입력하신 품목코드 '{target_item_code}'를 찾을 수 없습니다.</p>\", \"\"\n",
        "    except Exception as e: return f\"<h4>오류 발생</h4><p>{traceback.format_exc()}</p>\", \"\"\n",
        "\n",
        "### PoC #1: 유사 부서 상품 추천 함수 (HTML 출력으로 수정) ###\n",
        "def analyze_team_preferences(target_team_group: str):\n",
        "    \"\"\"\n",
        "    부서 그룹의 구매 랭킹을 HTML로 생성하여 반환하는 함수\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not target_team_group:\n",
        "            return \"분석할 부서 그룹을 선택해주세요.\"\n",
        "\n",
        "        # 품목 상세 정보가 담긴 딕셔너리\n",
        "        item_details_map = enriched_df.set_index('품목코드').to_dict('index')\n",
        "\n",
        "        # 1. 선택한 부서 그룹의 Top 5 구매 품목 계산\n",
        "        team_sales = orders_df[orders_df['부서그룹'] == target_team_group]\n",
        "        if team_sales.empty:\n",
        "            return f\"**[{target_team_group}]** 그룹의 구매 데이터가 없습니다.\"\n",
        "\n",
        "        top_items = team_sales.groupby('품목코드')['주문수량'].sum().nlargest(5).reset_index()\n",
        "\n",
        "        ranking_html = f\"<h4>**[{target_team_group}] 그룹 품목 구매 랭킹 (Top 5)**</h4>\"\n",
        "\n",
        "        for _, row in top_items.iterrows():\n",
        "            item_code = row['품목코드']\n",
        "            quantity = row['주문수량']\n",
        "\n",
        "            # 상품 마스터(enriched_df)에서 품목 상세 정보 가져오기\n",
        "            details = item_details_map.get(item_code, {})\n",
        "\n",
        "            item_name = details.get('한글품명', '알 수 없는 품목')\n",
        "            image_url = details.get('이미지_URL', '')\n",
        "            category = f\"{details.get('네이버_카테고리1', '')} > {details.get('네이버_카테고리2', '')}\"\n",
        "            spec = details.get('규격', '')\n",
        "            price = details.get('최저가', 0)\n",
        "\n",
        "            # HTML 생성\n",
        "            ranking_html += f\"\"\"\n",
        "            <div style=\"display:flex; align-items:center; border: 1px solid #E0E0E0; padding: 10px; border-radius: 5px; margin-bottom:10px;\">\n",
        "                <img src=\"{image_url}\" width=\"100\" style=\"margin-right:20px; border: 1px solid #DDD;\">\n",
        "                <div>\n",
        "                    <b><span style=\"color:#007BFF; font-weight:bold;\">{quantity}개</span> - [{item_code}] {item_name}</b><br>\n",
        "                    <b>카테고리:</b> {category}<br>\n",
        "                    <b>규격:</b> {spec}<br>\n",
        "                    <b>최저가(참고):</b> {price:,}원\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        return ranking_html\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"분석 중 오류 발생:\\n{traceback.format_exc()}\"\n",
        "\n",
        "### PoC #3: 리드 타임 예측 함수 (키 오류 수정) ###\n",
        "def predict_lead_time_poc3_all_cities(target_item_identifier: str):\n",
        "    if lead_time_model is None or hasher is None:\n",
        "        return \"모델이 로드되지 않아 예측할 수 없습니다.\"\n",
        "    if not target_item_identifier:\n",
        "        return \"분석할 품목을 선택 또는 입력해주세요.\"\n",
        "\n",
        "    try:\n",
        "        # --- 0. 기본 정보 준비 (모두 문자열로) ---\n",
        "        target_item_code_str = str(target_item_identifier.split(':')[0].strip()) if ':' in target_item_identifier else str(target_item_identifier.strip())\n",
        "        selected_item_series = enriched_df[enriched_df['품목코드'] == target_item_code_str].iloc[0]\n",
        "        item_name = selected_item_series['한글품명']\n",
        "        item_category = selected_item_series['네이버_카테고리1']\n",
        "\n",
        "        prediction_text = f\"### **[{item_name}]**의 전국 배송 예상 리드타임\\n\"\n",
        "        prediction_text += \"| 도시 | 예측 유형 | 배송유형 | 예상 소요일 |\\n\"\n",
        "        prediction_text += \"|:---|:---|:---|:---|\\n\"\n",
        "\n",
        "        # --- 1. 유사/카테고리 품목 목록 미리 준비 (모두 문자열) ---\n",
        "        target_embedding = item_collection.get(ids=[target_item_code_str], include=['embeddings'])['embeddings'][0]\n",
        "        similar_items_result = item_collection.query(query_embeddings=[target_embedding], n_results=50)\n",
        "\n",
        "        # ======== 수정된 부분: meta['id'] -> meta['품목코드'] ========\n",
        "        similar_item_codes = [meta['품목코드'] for meta in similar_items_result['metadatas'][0] if meta['품목코드'] != target_item_code_str]\n",
        "        # =========================================================\n",
        "\n",
        "        category_item_codes = enriched_df[enriched_df['네이버_카테고리1'] == item_category]['품목코드'].unique()\n",
        "\n",
        "        all_cities = sorted(delivery_df_processed['도시'].dropna().unique().tolist())\n",
        "\n",
        "        for city_name in all_cities:\n",
        "            has_result = False\n",
        "\n",
        "            # --- 1순위: 실제 배송 이력 조회 (문자열로 비교) ---\n",
        "            direct_history = delivery_df_processed[\n",
        "                (delivery_df_processed['품목코드'] == target_item_code_str) &\n",
        "                (delivery_df_processed['도시'] == city_name)\n",
        "            ]\n",
        "            used_delivery_types = direct_history['배송유형'].unique().tolist()\n",
        "\n",
        "            if used_delivery_types:\n",
        "                for delivery_type in used_delivery_types:\n",
        "                    input_data = {'품목코드': target_item_code_str, '배송유형': delivery_type, '도시': city_name, '주문_월': str(pd.Timestamp.now().month), '주문_요일': str(pd.Timestamp.now().dayofweek)}\n",
        "                    input_hashed = hasher.transform([input_data])\n",
        "                    predicted_days = lead_time_model.predict(input_hashed)[0]\n",
        "                    prediction_text += f\"| {city_name} | **실제 이력 기반** | {delivery_type} | **{max(0, predicted_days):.1f}일** |\\n\"\n",
        "                    has_result = True\n",
        "\n",
        "            else:\n",
        "                # --- 2순위: 유사 상품 배송 이력 조회 (문자열로 비교) ---\n",
        "                similar_history = delivery_df_processed[\n",
        "                    (delivery_df_processed['품목코드'].isin(similar_item_codes)) &\n",
        "                    (delivery_df_processed['도시'] == city_name)\n",
        "                ]\n",
        "                if not similar_history.empty:\n",
        "                    avg_lead_time = similar_history['리드타임'].mean()\n",
        "                    prediction_text += f\"| {city_name} | *유사 상품 기준* | - | *평균 {avg_lead_time:.1f}일* |\\n\"\n",
        "                    has_result = True\n",
        "\n",
        "                else:\n",
        "                    # --- 3순위: 동일 카테고리 기준 조회 (문자열로 비교) ---\n",
        "                    category_history = delivery_df_processed[\n",
        "                        (delivery_df_processed['품목코드'].isin(category_item_codes)) &\n",
        "                        (delivery_df_processed['도시'] == city_name)\n",
        "                    ]\n",
        "                    if not category_history.empty:\n",
        "                        avg_lead_time = category_history['리드타임'].mean()\n",
        "                        prediction_text += f\"| {city_name} | *카테고리 기준* | - | *평균 {avg_lead_time:.1f}일* |\\n\"\n",
        "                        has_result = True\n",
        "\n",
        "            # 4순위: 예측 불가\n",
        "            if not has_result:\n",
        "                prediction_text += f\"| {city_name} | 정보 없음 | - | - |\\n\"\n",
        "\n",
        "        return prediction_text\n",
        "\n",
        "    except IndexError:\n",
        "        return f\"<h4>오류</h4><p>입력하신 품목코드 '{target_item_code_str}'를 찾을 수 없습니다.</p>\"\n",
        "    except Exception as e:\n",
        "        return f\"예측 중 오류가 발생했습니다: {e}\\n\\n{traceback.format_exc()}\"\n",
        "\n",
        "print(\"✅ 모든 분석 함수 정의 완료.\")\n",
        "print(\"\\n✨ 2단계 성공! 다음 3단계(UI 실행) 코드를 실행해주세요.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xVQ6XeBE5Fq",
        "outputId": "424e651f-e47f-4ca7-d5a1-d1aeace60b76"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. 분석 함수 정의를 시작합니다...\n",
            "✅ 모든 분석 함수 정의 완료.\n",
            "\n",
            "✨ 2단계 성공! 다음 3단계(UI 실행) 코드를 실행해주세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import traceback\n",
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "\n",
        "# ======================================================================\n",
        "# 3. 모든 PoC 분석 함수 및 UI 정의\n",
        "# ======================================================================\n",
        "print(\"3. 분석 함수 정의 및 UI 실행을 시작합니다...\")\n",
        "\n",
        "# --- 분석 함수 정의 ---\n",
        "def calculate_cosine_similarity(query_embedding, result_embeddings):\n",
        "    query_vector = np.array(query_embedding); result_vectors = np.array(result_embeddings)\n",
        "    query_norm = np.linalg.norm(query_vector)\n",
        "    if query_norm == 0: return [0.0] * len(result_vectors)\n",
        "    query_vector /= query_norm\n",
        "    result_norms = np.linalg.norm(result_vectors, axis=1, keepdims=True)\n",
        "    result_vectors /= np.where(result_norms == 0, 1, result_norms)\n",
        "    return np.dot(result_vectors, query_vector).tolist()\n",
        "\n",
        "def analyze_item_similarity(target_item_identifier: str):\n",
        "    try:\n",
        "        if not target_item_identifier: return \"분석할 품목을 선택 또는 입력해주세요.\", \"\"\n",
        "        target_item_code_str = str(target_item_identifier.split(':')[0].strip()) if ':' in target_item_identifier else str(target_item_identifier.strip())\n",
        "        selected_item_series = enriched_df[enriched_df['품목코드'] == target_item_code_str].iloc[0]\n",
        "        target_category = selected_item_series['네이버_카테고리1']\n",
        "        target_embedding = item_collection.get(ids=[target_item_code_str], include=['embeddings'])['embeddings'][0]\n",
        "        similar_items_result = item_collection.query(query_embeddings=[target_embedding], n_results=15, include=['metadatas', 'embeddings'])\n",
        "        result_embeddings, result_metadatas = similar_items_result['embeddings'][0], similar_items_result['metadatas'][0]\n",
        "        similarity_scores = calculate_cosine_similarity(target_embedding, result_embeddings)\n",
        "        qualified_items = []\n",
        "        for score, meta in zip(similarity_scores, result_metadatas):\n",
        "            if meta['id'] != target_item_code_str and score >= 0.75:\n",
        "                item_details = enriched_df[enriched_df['품목코드'] == meta['id']].iloc[0].to_dict()\n",
        "                if item_details.get('네이버_카테고리1') == target_category:\n",
        "                    item_details['유사도'] = score\n",
        "                    qualified_items.append(item_details)\n",
        "        if not qualified_items: recommended_df = pd.DataFrame()\n",
        "        else: recommended_df = pd.DataFrame(qualified_items).sort_values(by='유사도', ascending=False).head(5)\n",
        "        selected_item_html = f\"\"\"<h4>선택된 품목 정보</h4><div style=\"display:flex; align-items:center; border: 1px solid #E0E0E0; padding: 10px; border-radius: 5px;\"><img src=\"{selected_item_series.get('이미지_URL', '')}\" width=\"100\" style=\"margin-right:20px; border: 1px solid #DDD;\"><div><b>{selected_item_series.get('한글품명', '')} ({selected_item_series.get('품목코드', '')})</b><br><b>카테고리:</b> {selected_item_series.get('네이버_카테고리1', '')} > {selected_item_series.get('네이버_카테고리2', '')}<br><b>규격:</b> {selected_item_series.get('규격', '')}</div></div>\"\"\"\n",
        "        recommend_html = \"<h4>AI 추천 유사 품목</h4>\"\n",
        "        if recommended_df.empty: recommend_html += f\"<p>'{target_category}' 카테고리 내에서 기준점수(0.75)를 넘는 유사 품목을 찾을 수 없습니다.</p>\"\n",
        "        else:\n",
        "            for _, row in recommended_df.iterrows():\n",
        "                recommend_html += f\"\"\"<div style=\"display:flex; align-items:center; border: 1px solid #4CAF50; padding: 10px; border-radius: 5px; margin-bottom:10px;\"><img src=\"{row.get('이미지_URL', '')}\" width=\"100\" style=\"margin-right:20px; border: 1px solid #DDD;\"><div><b>{row.get('한글품명', '')} ({row.get('품목코드', '')}) - <span style=\"color:#4CAF50; font-weight:bold;\">유사도: {row.get('유사도', 0):.2f}</span></b><br><b>카테고리:</b> {row.get('네이버_카테고리1', '')} > {row.get('네이버_카테고리2', '')}<br><b>규격:</b> {row.get('규격', '')}</div></div>\"\"\"\n",
        "        return selected_item_html, recommend_html\n",
        "    except IndexError: return f\"<h4>오류</h4><p>입력하신 품목코드 '{target_item_code_str}'를 찾을 수 없습니다.</p>\", \"\"\n",
        "    except Exception as e: return f\"<h4>오류 발생</h4><p>{traceback.format_exc()}</p>\", \"\"\n",
        "\n",
        "def analyze_team_preferences(target_team_group: str):\n",
        "    try:\n",
        "        if not target_team_group: return \"분석할 부서 그룹을 선택해주세요.\"\n",
        "        item_code_to_name_map = pd.Series(enriched_df.한글품명.values, index=enriched_df.품목코드).to_dict()\n",
        "        team_sales = orders_df[orders_df['부서그룹'] == target_team_group]\n",
        "        if team_sales.empty: return f\"**[{target_team_group}]** 그룹의 구매 데이터가 없습니다.\"\n",
        "        top_items = team_sales.groupby('품목코드')['주문수량'].sum().nlargest(5).reset_index()\n",
        "        top_items['품목명'] = top_items['품목코드'].map(item_code_to_name_map)\n",
        "        ranking_text = f\"**[{target_team_group}] 그룹 품목 구매 랭킹 (Top 5)**\\n\"\n",
        "        for _, row in top_items.iterrows():\n",
        "            ranking_text += f\"- [{row.get('품목코드')}] {row.get('품목명', '이름없음')} ({row.get('주문수량', 0)}개)\\n\"\n",
        "        return ranking_text\n",
        "    except Exception as e: return f\"분석 중 오류 발생:\\n{traceback.format_exc()}\"\n",
        "\n",
        "def predict_lead_time_poc3(item_name, city_name):\n",
        "    if lead_time_model is None or hasher is None: return \"모델이 로드되지 않아 예측할 수 없습니다.\"\n",
        "    if not item_name or not city_name: return \"품목과 도시를 모두 선택해주세요.\"\n",
        "    try:\n",
        "        item_code_str = enriched_df.loc[enriched_df['한글품명'] == item_name, '품목코드'].iloc[0]\n",
        "        prediction_text = f\"### **[{item_name}]**의 **[{city_name}]** 예상 리드타임\\n\"\n",
        "        for delivery_type in ['배송', '직납', '집배송']:\n",
        "            input_data = {'품목코드': item_code_str, '배송유형': delivery_type, '도시': city_name, '주문_월': str(pd.Timestamp.now().month), '주문_요일': str(pd.Timestamp.now().dayofweek)}\n",
        "            input_hashed = hasher.transform([input_data])\n",
        "            predicted_days = lead_time_model.predict(input_hashed)[0]\n",
        "            prediction_text += f\"- **{delivery_type}**: {max(0, predicted_days):.1f}일 소요 예상\\n\"\n",
        "        target_embedding = item_collection.get(ids=[item_code_str], include=['embeddings'])['embeddings'][0]\n",
        "        similar_items_result = item_collection.query(query_embeddings=[target_embedding], n_results=50)\n",
        "        similar_item_codes = [meta['id'] for meta in similar_items_result['metadatas'][0] if meta['id'] != item_code_str]\n",
        "        similar_history = delivery_df_processed[(delivery_df_processed['품목코드'].isin(similar_item_codes)) & (delivery_df_processed['도시'] == city_name)]\n",
        "        if not similar_history.empty:\n",
        "            avg_lead_time = similar_history['리드타임'].mean()\n",
        "            prediction_text += f\"\\n---\\n*참고: 이 도시로 배송된 **유사 상품들은 평균 {avg_lead_time:.1f}일**이 소요되었습니다.*\"\n",
        "        else:\n",
        "            item_category = enriched_df.loc[enriched_df['품목코드'] == item_code_str, '네이버_카테고리1'].iloc[0]\n",
        "            category_item_codes = enriched_df[enriched_df['네이버_카테고리1'] == item_category]['품목코드'].unique()\n",
        "            category_history = delivery_df_processed[(delivery_df_processed['품목코드'].isin(category_item_codes)) & (delivery_df_processed['도시'] == city_name)]\n",
        "            if not category_history.empty:\n",
        "                avg_lead_time = category_history['리드타임'].mean()\n",
        "                prediction_text += f\"\\n---\\n*참고: 이 도시의 **동일 카테고리 상품들은 평균 {avg_lead_time:.1f}일**이 소요되었습니다.*\"\n",
        "            else:\n",
        "                prediction_text += \"\\n---\\n*참고: 이 도시로 배송된 유사/카테고리 상품 이력이 없습니다.*\"\n",
        "        return prediction_text\n",
        "    except Exception as e: return f\"예측 중 오류 발생: {e}\\n\\n{traceback.format_exc()}\"\n",
        "\n",
        "# --- Gradio UI 준비 및 실행 ---\n",
        "print(\"\\n3. Gradio UI를 실행합니다...\")\n",
        "teams_list = orders_df['부서그룹'].value_counts().index.tolist()\n",
        "categories_list = sorted(enriched_df['네이버_카테고리1'].dropna().unique().tolist())\n",
        "enriched_df['item_display'] = enriched_df['품목코드'] + \" : \" + enriched_df['한글품명']\n",
        "all_cities_list = sorted(delivery_df_processed['도시'].dropna().unique().tolist()) if 'delivery_df_processed' in locals() and delivery_df_processed is not None else []\n",
        "all_items_list = sorted(enriched_df['한글품명'].dropna().unique().tolist())\n",
        "def update_item_dropdown(category):\n",
        "    if not category: return gr.Dropdown(choices=[])\n",
        "    filtered_df = enriched_df[enriched_df['네이버_카테고리1'] == category]\n",
        "    choices = sorted(filtered_df['item_display'].dropna().unique().tolist())\n",
        "    return gr.Dropdown(choices=choices, value=choices[0] if choices else None)\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# 🤖 IMK Plus AI PoC Project (통합)\")\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"PoC #1: 유사 부서 상품 추천\"):\n",
        "            gr.Markdown(\"특정 부서 그룹을 선택하면, 해당 그룹의 품목 구매 랭킹 Top 5를 보여줍니다.\")\n",
        "            with gr.Row():\n",
        "                team_dropdown = gr.Dropdown(choices=teams_list, label=\"분석할 부서 그룹을 선택하세요\")\n",
        "                poc1_button = gr.Button(\"분석 실행\", variant=\"primary\")\n",
        "            ranking_output = gr.Textbox(label=\"선택한 그룹의 품목 구매 랭킹\", lines=7)\n",
        "            poc1_button.click(fn=analyze_team_preferences, inputs=team_dropdown, outputs=ranking_output)\n",
        "        with gr.TabItem(\"PoC #2: 품목별 유사 상품 추천\"):\n",
        "            gr.Markdown(\"AI가 **유사도 0.75 이상**인 품목 중 가장 유사한 **최대 5개**를 추천합니다.\")\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"카테고리로 찾기\"):\n",
        "                    with gr.Row():\n",
        "                        category_dd = gr.Dropdown(choices=categories_list, label=\"1. 대분류 카테고리를 선택하세요\")\n",
        "                        item_dd = gr.Dropdown(label=\"2. 품목을 선택하세요 (카테고리 먼저 선택)\", interactive=True)\n",
        "                    cat_button = gr.Button(\"유사 품목 찾기\", variant=\"primary\")\n",
        "                with gr.TabItem(\"직접 검색하기\"):\n",
        "                    item_code_tb = gr.Textbox(label=\"품목코드를 직접 입력하세요\", placeholder=\"예: 1419128400\")\n",
        "                    code_button = gr.Button(\"유사 품목 찾기\", variant=\"primary\")\n",
        "            gr.HTML(\"<hr style='margin: 20px 0;'>\")\n",
        "            selected_item_output = gr.HTML()\n",
        "            similar_items_output = gr.HTML()\n",
        "            category_dd.change(fn=update_item_dropdown, inputs=category_dd, outputs=item_dd)\n",
        "            cat_button.click(fn=analyze_item_similarity, inputs=item_dd, outputs=[selected_item_output, similar_items_output])\n",
        "            code_button.click(fn=analyze_item_similarity, inputs=item_code_tb, outputs=[selected_item_output, similar_items_output])\n",
        "        with gr.TabItem(\"PoC #3: 배송 리드타임 예측\"):\n",
        "            gr.Markdown(\"품목을 선택하면, AI가 **전국 도시별** 예상 배송 소요 시간을 '폭포수 모델'로 예측합니다.\")\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"카테고리로 찾기\"):\n",
        "                    with gr.Row():\n",
        "                        poc3_category_dd = gr.Dropdown(choices=categories_list, label=\"1. 대분류 카테고리를 선택하세요\")\n",
        "                        poc3_item_dd = gr.Dropdown(label=\"2. 품목을 선택하세요 (카테고리 먼저 선택)\", interactive=True)\n",
        "                    poc3_cat_button = gr.Button(\"전국 리드타임 조회\", variant=\"primary\")\n",
        "                with gr.TabItem(\"직접 검색하기\"):\n",
        "                    poc3_item_code_tb = gr.Textbox(label=\"품목코드를 직접 입력하세요\", placeholder=\"예: 1419128400\")\n",
        "                    poc3_code_button = gr.Button(\"전국 리드타임 조회\", variant=\"primary\")\n",
        "            gr.HTML(\"<hr style='margin: 20px 0;'>\")\n",
        "            poc3_output = gr.Markdown()\n",
        "\n",
        "            poc3_category_dd.change(fn=update_item_dropdown, inputs=poc3_category_dd, outputs=poc3_item_dd)\n",
        "            poc3_cat_button.click(fn=predict_lead_time_poc3_all_cities, inputs=poc3_item_dd, outputs=poc3_output)\n",
        "            poc3_code_button.click(fn=predict_lead_time_poc3_all_cities, inputs=poc3_item_code_tb, outputs=poc3_output)\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "Ddl0MnvpmGtD",
        "outputId": "f1995a0c-3828-4526-d31d-a76e74510f1d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. 분석 함수 정의 및 UI 실행을 시작합니다...\n",
            "\n",
            "3. Gradio UI를 실행합니다...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://8c3d1723281398377a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8c3d1723281398377a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://8c3d1723281398377a.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"PoC #3 시연에 적합한, '배송지가 다양한' 상위 5개 품목을 찾습니다...\")\n",
        "print(\"(모든 품목코드를 '문자열'로 통일하여 검증합니다)\")\n",
        "\n",
        "try:\n",
        "    # --- 1. 모든 품목코드를 '문자열'로 통일 ---\n",
        "    enriched_df['품목코드'] = enriched_df['품목코드'].astype(str)\n",
        "    delivery_df_processed['품목코드'] = delivery_df_processed['품목코드'].astype(str)\n",
        "\n",
        "    # --- 2. '진짜' 품목코드(상품 마스터에 있는) 기록만 필터링 ---\n",
        "    valid_item_codes = set(enriched_df['품목코드'].unique())\n",
        "    filtered_delivery_df = delivery_df_processed[\n",
        "        delivery_df_processed['품목코드'].isin(valid_item_codes)\n",
        "    ]\n",
        "\n",
        "    # --- 3. 품목별 고유 도시 '수' 계산 ---\n",
        "    city_diversity_count = filtered_delivery_df.groupby('품목코드')['도시'].nunique().sort_values(ascending=False)\n",
        "\n",
        "    # --- 4. 품목별 고유 도시 '목록' 생성 (신규 추가) ---\n",
        "    city_diversity_list = filtered_delivery_df.groupby('품목코드')['도시'].apply(lambda x: list(x.unique()))\n",
        "\n",
        "    # --- 5. 품목명 결합 ---\n",
        "    item_names_df = enriched_df[['품목코드', '한글품명']]\n",
        "\n",
        "    # --- 6. (도시 수) + (도시 목록) + (품목명) 데이터 결합 ---\n",
        "    demo_candidates = pd.merge(\n",
        "        city_diversity_count.reset_index(name='배송된_도시_수'),\n",
        "        city_diversity_list.reset_index(name='배송된_도시_목록'),\n",
        "        on='품목코드',\n",
        "        how='left'\n",
        "    )\n",
        "    demo_candidates = pd.merge(\n",
        "        demo_candidates,\n",
        "        item_names_df,\n",
        "        on='품목코드',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # --- 7. 최종 결과 출력 ---\n",
        "    print(\"\\n[PoC #3 시연 추천 품목 Top 5 (상세 도시 목록 포함)]\")\n",
        "    # '품목코드', '한글품명', '배송된_도시_수', '배송된_도시_목록' 순서로 정렬\n",
        "    final_columns = ['품목코드', '한글품명', '배송된_도시_수', '배송된_도시_목록']\n",
        "    display(demo_candidates[final_columns].head(5))\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\n❌ [에러] 데이터가 메모리에 없습니다.\")\n",
        "    print(\"   - '1&2단계 통합 코드'를 먼저 실행해주세요.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ [에러] 분석 중 오류가 발생했습니다: {e}\")"
      ],
      "metadata": {
        "id": "mK3T5Lj5gm99"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO5kvmIOEX0kbZNLFCW3Zem",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08d598cdc5ce4b42b1903c96edea593f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d277a2ce92b84520960d6b78ef223f97",
              "IPY_MODEL_5ff5d99454b844c3876a2dc786ca8836",
              "IPY_MODEL_3ad87978438a4bf2a6a8e29905346b58"
            ],
            "layout": "IPY_MODEL_c9a9bd4871f2453681d3da5465ef18a1"
          }
        },
        "d277a2ce92b84520960d6b78ef223f97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c742b88896249678ee3dad4dddb5e51",
            "placeholder": "​",
            "style": "IPY_MODEL_878dfd4d7f0e46438f8331ca5774825e",
            "value": "Batches: 100%"
          }
        },
        "5ff5d99454b844c3876a2dc786ca8836": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebfc2e8c8e524d8784b13e9b40c6bed2",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1642bc754d0c45a5837f63c2d031439b",
            "value": 26
          }
        },
        "3ad87978438a4bf2a6a8e29905346b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2935f090be714175988b1ae1a40a85c0",
            "placeholder": "​",
            "style": "IPY_MODEL_d2d2913c9593404da50883dd6a0eefb0",
            "value": " 26/26 [00:46&lt;00:00,  1.16s/it]"
          }
        },
        "c9a9bd4871f2453681d3da5465ef18a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c742b88896249678ee3dad4dddb5e51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878dfd4d7f0e46438f8331ca5774825e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebfc2e8c8e524d8784b13e9b40c6bed2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1642bc754d0c45a5837f63c2d031439b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2935f090be714175988b1ae1a40a85c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2d2913c9593404da50883dd6a0eefb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c39223f8ea245ef8144a736e9fe92e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5a305cad59d44598f8f6a308d0d433d",
              "IPY_MODEL_9ff2fffa5c69436388dc98f9e13c9ef2",
              "IPY_MODEL_f7d85e78d3634b88948741d39ae7f472"
            ],
            "layout": "IPY_MODEL_05428539762146f882a87fd02bdb617c"
          }
        },
        "a5a305cad59d44598f8f6a308d0d433d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9377f5c94e7649d19929e759844e018b",
            "placeholder": "​",
            "style": "IPY_MODEL_9014c49cd3a040fca029c2b92b250a18",
            "value": "Batches: 100%"
          }
        },
        "9ff2fffa5c69436388dc98f9e13c9ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d100a0a4a9b407a8b0b0e6c2d777e6d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c22ce4e1b204cb6b946400972a7e168",
            "value": 1
          }
        },
        "f7d85e78d3634b88948741d39ae7f472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e35e73cb2a334a5d96e8aa6c2a7b969d",
            "placeholder": "​",
            "style": "IPY_MODEL_f83e710f667a42f2aa9a31b0f8c86eea",
            "value": " 1/1 [00:01&lt;00:00,  1.46s/it]"
          }
        },
        "05428539762146f882a87fd02bdb617c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9377f5c94e7649d19929e759844e018b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9014c49cd3a040fca029c2b92b250a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d100a0a4a9b407a8b0b0e6c2d777e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c22ce4e1b204cb6b946400972a7e168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e35e73cb2a334a5d96e8aa6c2a7b969d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83e710f667a42f2aa9a31b0f8c86eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}